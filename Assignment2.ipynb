{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Comparing Tokenization Methods and using Hyperparameter Optimization for IMDB Classification\n",
    "\n",
    "### Maxim Ryabinov (U02204083)\n",
    "### CAP4641: Natural Language Processing \n",
    "### Instructor: Dr. Ankur Mali \n",
    "### University of South Florida (Spring 2025)\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Initial Setup\n",
    "- Imports the necessary libraries and sets random seeds to ensure reproducibility later on.\n",
    "- Defines a set of storage classes that will be helpful during model testing.\n",
    "- A few tokenizer and preprocessing utility functions are also defined here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Suppress TensorFlow warnings.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Imports all libraries used within the notebook.\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import random\n",
    "\n",
    "# Sets the seeds for reproducibility.\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Data Storage Classes\n",
    "# --------------------\n",
    "# Stores a set of hyperparameters, as well as creates random sample hyperparameters.\n",
    "class Hyperparameters():    \n",
    "    def __init__(self, learning_rate=None, hidden_layers=None, hidden_sizes=None, batch_size=None, optimizer=None, activation_function=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.activation_function = activation_function\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (f\"Hyperparameters:\\n\"\n",
    "                f\"Learning Rate: {self.learning_rate:.4f} | Num Layers: {self.hidden_layers} | \"\n",
    "                f\"Hidden Sizes: {self.hidden_sizes} | Batch Size: {self.batch_size} | \"\n",
    "                f\"Optimizer: {self.optimizer.__name__} | Activation Function: {self.activation_function.__name__}\\n\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_hyperparameter_choices():\n",
    "        # Returns a dictionary of possible hyperparameter values.\n",
    "        return {\n",
    "            \"learning_rates\": [0.001, 0.0005, 0.0001],\n",
    "            \"hidden_layers\": [1, 2, 3],\n",
    "            \"hidden_sizes\": [128, 256, 512],\n",
    "            \"batch_sizes\": [32, 64, 128],\n",
    "            \"optimizers\": [tf.keras.optimizers.Adam, tf.keras.optimizers.SGD, tf.keras.optimizers.RMSprop],\n",
    "            \"activation_functions\": [tf.nn.relu, tf.nn.tanh, tf.nn.leaky_relu]\n",
    "        }\n",
    "        \n",
    "    def sample_random(self):        \n",
    "        choices = self.get_hyperparameter_choices()\n",
    "        self.learning_rate = random.choice(choices[\"learning_rates\"])\n",
    "        self.hidden_layers = random.choice(choices[\"hidden_layers\"])\n",
    "        picked_size = random.choice(choices[\"hidden_sizes\"])\n",
    "        self.hidden_sizes = [picked_size] * self.hidden_layers\n",
    "        self.batch_size = random.choice(choices[\"batch_sizes\"])\n",
    "        self.optimizer = random.choice(choices[\"optimizers\"])\n",
    "        self.activation_function = random.choice(choices[\"activation_functions\"])\n",
    "\n",
    "# Stores the performance metrics of a model.\n",
    "class PerformanceMetrics():\n",
    "    def __init__(self, loss, accuracy, precision, recall):\n",
    "        self.loss = loss\n",
    "        self.accuracy = accuracy\n",
    "        self.precision = precision\n",
    "        self.recall = recall\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (f\"Loss: {self.loss:.4f} | Accuracy: {self.accuracy:.4f} | \"\n",
    "                f\"Precision: {self.precision:.4f} | Recall: {self.recall:.4f}\\n\")\n",
    "\n",
    "# Stores the hyperparameters and performance metrics of a model.\n",
    "class Result():\n",
    "    def __init__(self, hyperparameters, performance_metrics):\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.performance_metrics = performance_metrics\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (f\"Hyperparameters:\\n\"\n",
    "                f\"Learning Rate: {self.hyperparameters.learning_rate:.4f} | Num Layers: {self.hyperparameters.hidden_layers} | \"\n",
    "                f\"Hidden Sizes: {self.hyperparameters.hidden_sizes} | Batch Size: {self.hyperparameters.batch_size} | \"\n",
    "                f\"Optimizer: {self.hyperparameters.optimizer.__name__} | Activation Function: {self.hyperparameters.activation_function.__name__}\\n\"\n",
    "                f\"\\nPerformance Metrics:\\n\"\n",
    "                f\"Loss: {self.performance_metrics.loss:.4f} | Accuracy: {self.performance_metrics.accuracy:.4f} | \"\n",
    "                f\"Precision: {self.performance_metrics.precision:.4f} | Recall: {self.performance_metrics.recall:.4f}\\n\")\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Tokenizer and Preprocessing Util Functions\n",
    "# ------------------------------------------\n",
    "# Creates a fitted tokenizer object, can either be character or word level.\n",
    "def create_tokenizer(texts, is_char_level, num_words=None):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=is_char_level, lower=True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "# Converts texts to a bag-of-words representation.\n",
    "def texts_to_bow(tokenizer, texts):\n",
    "    # Texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
    "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "    return matrix # Binary bag of characters/words.\n",
    "\n",
    "# Converts texts to a sequence of integers.\n",
    "def one_hot_encode(labels, num_classes=2):\n",
    "    return np.eye(num_classes)[labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Defining the MLP Model\n",
    "This is where the main MLP model is defined for text classification on the IMDB dataset.\n",
    "\n",
    "It is able to take a range of parameters including:\n",
    "- The number of features: `size_input`\n",
    "- The number of hidden layers: `num_hidden`\n",
    "- A list of sizes (# of neurons) in each layer: `hidden_sizes`\n",
    "- The size of the output layer: `output_size`\n",
    "- Amount of training samples used: `batch_size`\n",
    "- The optimizer's learning rate: `learning_rate`\n",
    "- Optimizer being used: `optimizer`\n",
    "- Activation Function used: `act_func`\n",
    "- (Optionally) Device being used: `device`\n",
    "\n",
    "Additionally, the model contains methods for forward and back propagation, calculating the loss, and computing the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# MLP Class Definition\n",
    "# --------------------\n",
    "class MLP(object):\n",
    "    def __init__(self, size_input, num_hidden, hidden_sizes, size_output, batch_size, optimizer, learning_rate, act_func, device=None):\n",
    "        self.size_input = size_input # Number of features\n",
    "        self.num_hidden = num_hidden # Number of hidden layers\n",
    "        self.hidden_sizes = hidden_sizes # List of hidden layer sizes\n",
    "        self.size_output = size_output # Number of classes (Binary Classification)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer # Optimizer class (e.g. Adam, SGD, RMSprop)\n",
    "        self.act_func = act_func # Activation function to use (e.g. ReLU, Tanh, Leaky ReLU)\n",
    "        self.device = device # Device to run the model on (cpu or gpu)\n",
    "        \n",
    "        # Holds the model's weights and biases.\n",
    "        self.variables = []\n",
    "        \n",
    "        # Initializes the model's weights and biases.\n",
    "        w = tf.Variable(tf.random.normal([self.size_input, self.hidden_sizes[0]], stddev=0.1))\n",
    "        b = tf.Variable(tf.zeros([1, self.hidden_sizes[0]]))\n",
    "        self.variables.append((w, b))\n",
    "        \n",
    "        # Initializes the weights and biases for the hidden layers.\n",
    "        for i in range(1, self.num_hidden):\n",
    "            w = tf.Variable(tf.random.normal([self.hidden_sizes[i-1], self.hidden_sizes[i]], stddev=0.1))\n",
    "            b = tf.Variable(tf.zeros([1, self.hidden_sizes[i]]))\n",
    "            self.variables.append((w, b))\n",
    "        \n",
    "        # Initializes the weights and biases for the output layer.\n",
    "        w = tf.Variable(tf.random.normal([self.hidden_sizes[-1], self.size_output], stddev=0.1))\n",
    "        b = tf.Variable(tf.zeros([1, self.size_output]))\n",
    "        self.variables.append((w, b))\n",
    "\n",
    "    # Computes the output tensor during the forward pass. X is the input tensor.\n",
    "    def forward(self, X):\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "        return self.y\n",
    "\n",
    "    # Computes the loss value given the predicted and actual output tensors.\n",
    "    def loss(self, y_pred, y_true):\n",
    "        # Tensor of shape (batch_size, size_output)\n",
    "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        loss_x = cce(y_true_tf, y_pred_tf)\n",
    "        return loss_x\n",
    "\n",
    "    # Computes and applies gradients of the loss with respect to the variables (weights and bias).\n",
    "    def backward(self, X_train, y_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "        \n",
    "        trainable_vars = [var for w, b in self.variables for var in (w, b)]    \n",
    "        \n",
    "        grads = tape.gradient(current_loss, trainable_vars)\n",
    "        \n",
    "        optimizer = self.optimizer(learning_rate=self.learning_rate)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "    # Custom method to compute the output tensor during the forward pass.\n",
    "    def compute_output(self, X):  \n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        z = X_tf\n",
    "        \n",
    "        # Goes through each hidden layer and applies the weights and biases\n",
    "        for w, b in self.variables:\n",
    "            z = tf.matmul(z, w) + b\n",
    "            \n",
    "            # If the current layer is the last layer, then its the output layer.\n",
    "            if (w, b) == self.variables[-1]:\n",
    "                return z\n",
    "            \n",
    "            z = self.act_func(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training the Model (Train Loop)\n",
    "\n",
    "This function simply defines the training process for the model. It will train the model multiple times depending on the number of epochs (three by default).\n",
    "\n",
    "The function also outputs relevant data to the training process:\n",
    "- Current Epoch\n",
    "- Training Loss\n",
    "- Validation Loss\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, X_val, y_train, y_val, epochs = 3):\n",
    "    num_batches = int(np.ceil(X_train.shape[0] / model.batch_size))\n",
    "\n",
    "    print(\"\\nStarting training...\\n\")\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data at the start of each epoch.\n",
    "        indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            start = i * model.batch_size\n",
    "            end = min((i+1) * model.batch_size, X_train.shape[0])\n",
    "            X_batch = X_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            # Compute gradients and update weights.\n",
    "            predictions = model.forward(X_batch)\n",
    "            loss_value = model.loss(predictions, y_batch)\n",
    "            model.backward(X_batch, y_batch)\n",
    "\n",
    "            epoch_loss += loss_value.numpy() * (end - start)\n",
    "\n",
    "        epoch_loss /= X_train.shape[0]\n",
    "\n",
    "        # Evaluate on validation set.\n",
    "        val_logits = model.forward(X_val)\n",
    "        val_loss = model.loss(val_logits, y_val).numpy()\n",
    "        val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
    "        true_val = np.argmax(y_val, axis=1)\n",
    "        accuracy = np.mean(val_preds == true_val)\n",
    "        precision = precision_score(true_val, val_preds)\n",
    "        recall = recall_score(true_val, val_preds)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "            f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation on Test Set (Test Loop)\n",
    "\n",
    "This function takes a trained model and evaluates its performance based on a test portion of the IMDB dataset.\n",
    "\n",
    "The function also outputs the following performance metrics:\n",
    "- Loss\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_test, y_test):\n",
    "    # -------------------------------\n",
    "    # Final Evaluation on Test Set\n",
    "    # -------------------------------\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_logits = model.forward(X_test)\n",
    "    test_loss = model.loss(test_logits, y_test).numpy()\n",
    "    test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
    "    true_test = np.argmax(y_test, axis=1)\n",
    "    test_accuracy = np.mean(test_preds == true_test)\n",
    "    test_precision = precision_score(true_test, test_preds)\n",
    "    test_recall = recall_score(true_test, test_preds)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "        f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n",
    "    \n",
    "    return test_loss, test_accuracy, test_precision, test_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Loading IMDB Dataset and Tokenizing\n",
    "This cell loads the IMDB dataset into the notebook.\n",
    "\n",
    "It performs both character-level tokenization and word-level tokenization.\n",
    "\n",
    "The output below logs data allocation for the training and test datasets, as well as information on vocabulary sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n",
      "\n",
      "IMBD Dataset loaded:\n",
      "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
      "\n",
      "Tokenizing texts...\n",
      "Char tokenizer vocabulary size: 134\n",
      "Word tokenizer vocabulary size: 80169 \n",
      "\n",
      "Converting texts to bag-of-characters...\n",
      "Converting texts to bag-of-words...\n",
      "Performing one-hot encoding on labels...\n",
      "\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# Load and Prepare the IMDB Dataset\n",
    "# ---------------------------------\n",
    "print(\"Loading IMDB dataset...\\n\")\n",
    "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
    "                                           split=['train', 'test'],\n",
    "                                           as_supervised=True,\n",
    "                                           with_info=True)\n",
    "\n",
    "# Convert training dataset to lists.\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for text, label in tfds.as_numpy(ds_train):\n",
    "    # Decode byte strings to utf-8 strings.\n",
    "    train_texts.append(text.decode('utf-8'))\n",
    "    train_labels.append(label)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Create a validation set from the training data (20% for validation).\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert test dataset to lists.\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for text, label in tfds.as_numpy(ds_test):\n",
    "    test_texts.append(text.decode('utf-8'))\n",
    "    test_labels.append(label)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(\"IMBD Dataset loaded:\")\n",
    "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\\n\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Preprocessing: Tokenization and Vectorization\n",
    "# ---------------------------------------------\n",
    "print(\"Tokenizing texts...\")\n",
    "# Build the character-level tokenizer on the training texts.\n",
    "char_tokenizer = create_tokenizer(train_texts, is_char_level=True)\n",
    "print(\"Char tokenizer vocabulary size:\", len(char_tokenizer.word_index) + 1)\n",
    "\n",
    "word_tokenizer = create_tokenizer(train_texts, is_char_level=False, num_words=10000) # Limited to the top 10,000 common words.\n",
    "print(\"Word tokenizer vocabulary size:\", len(word_tokenizer.word_index) + 1, \"\\n\")\n",
    "\n",
    "print(\"Converting texts to bag-of-characters...\")\n",
    "# Convert texts to bag-of-characters representation.\n",
    "X_train_char = texts_to_bow(char_tokenizer, train_texts)\n",
    "X_val_char   = texts_to_bow(char_tokenizer, val_texts)\n",
    "X_test_char  = texts_to_bow(char_tokenizer, test_texts)\n",
    "\n",
    "print(\"Converting texts to bag-of-words...\")\n",
    "# Convert texts to bag-of-words representation.\n",
    "X_train_word = texts_to_bow(word_tokenizer, train_texts)\n",
    "X_val_word   = texts_to_bow(word_tokenizer, val_texts)\n",
    "X_test_word  = texts_to_bow(word_tokenizer, test_texts)\n",
    "\n",
    "print(\"Performing one-hot encoding on labels...\\n\")\n",
    "# Convert labels to one-hot encoding.\n",
    "y_train = one_hot_encode(train_labels)\n",
    "y_val   = one_hot_encode(val_labels)\n",
    "y_test  = one_hot_encode(test_labels)\n",
    "print(\"Preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Tokenization Vs. Word Tokenization\n",
    "This is where character-level and word-level tokenization is compared.\n",
    "\n",
    "Both models are created, and then trained three times each for robustness. Validation metrics are also logged.\n",
    "\n",
    "Based on the performance of both models (accuracy), word-level tokenization performs marginally better than character-level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the character-level model...\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6755 | Val Loss: 0.6853 | Accuracy: 0.5690 | Precision: 0.5351 | Recall: 0.8449\n",
      "Epoch 02 | Training Loss: 0.6696 | Val Loss: 0.6654 | Accuracy: 0.6040 | Precision: 0.6076 | Recall: 0.5173\n",
      "Epoch 03 | Training Loss: 0.6660 | Val Loss: 0.6642 | Accuracy: 0.6020 | Precision: 0.6073 | Recall: 0.5066\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.6656 | Test Accuracy: 0.6004 | Test Precision: 0.6217 | Test Recall: 0.5126\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Training the word-level model...\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4478 | Val Loss: 0.3239 | Accuracy: 0.8728 | Precision: 0.8449 | Recall: 0.9035\n",
      "Epoch 02 | Training Loss: 0.2638 | Val Loss: 0.3049 | Accuracy: 0.8806 | Precision: 0.9069 | Recall: 0.8399\n",
      "Epoch 03 | Training Loss: 0.2221 | Val Loss: 0.3365 | Accuracy: 0.8764 | Precision: 0.9228 | Recall: 0.8131\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3572 | Test Accuracy: 0.8670 | Test Precision: 0.9304 | Test Recall: 0.7933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float32(0.35722524), np.float64(0.86696), 0.9303809345092888, 0.79328)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seeds for reproducibility.\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "# -----------\n",
    "# Model Setup\n",
    "# -----------\n",
    "# The input size is determined by the dimension of the bag-of-characters vector.\n",
    "char_size_input = X_train_char.shape[1]\n",
    "word_size_input = X_train_word.shape[1]\n",
    "\n",
    "# Parameters used for both models.\n",
    "num_hidden = 3 # Number of hidden layers\n",
    "hidden_sizes = [128, 64, 32] # Size of each hidden layer\n",
    "batch_size = 128\n",
    "optimizer = tf.keras.optimizers.Adam\n",
    "learning_rate = 0.001\n",
    "act_func = tf.nn.relu # Activation function used\n",
    "size_output  = 2 # Binary classification\n",
    "\n",
    "# Create the character-level and word-level models.\n",
    "char_model = MLP(size_input=char_size_input,\n",
    "            num_hidden=num_hidden,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            size_output=size_output,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=optimizer,\n",
    "            learning_rate=learning_rate,\n",
    "            act_func=act_func,\n",
    "            device=None)\n",
    "\n",
    "word_model = MLP(size_input=word_size_input,\n",
    "            num_hidden=num_hidden,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            size_output=size_output,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=optimizer,\n",
    "            learning_rate=learning_rate,\n",
    "            act_func=act_func,\n",
    "            device=None)\n",
    "\n",
    "# --------------------------------\n",
    "# Training and Evaluation (Testing)\n",
    "# --------------------------------\n",
    "\n",
    "print(\"\\nTraining the character-level model...\")\n",
    "train_model(char_model, X_train_char, X_val_char, y_train, y_val)\n",
    "test_model(char_model, X_test_char, y_test)\n",
    "\n",
    "print(\"\\n-----------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"\\nTraining the word-level model...\")\n",
    "train_model(word_model, X_train_word, X_val_word, y_train, y_val)\n",
    "test_model(word_model, X_test_word, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter Optimization\n",
    "\n",
    "This is where the process of hyper-parameter optimization occurs. The model is run through 50 different hyper-parameter configuations, each one being trained multiple times for robustness.\n",
    "\n",
    "The results for each set of hyper-parameters is then saved into the data storage classes defined during the intial setup so they can be more easily processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 3 | Hidden Sizes: [256, 256, 256] | Batch Size: 32 | Optimizer: Adam | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.5107 | Val Loss: 0.4811 | Accuracy: 0.8502 | Precision: 0.8042 | Recall: 0.9134\n",
      "Epoch 02 | Training Loss: 0.4700 | Val Loss: 0.5042 | Accuracy: 0.8736 | Precision: 0.8604 | Recall: 0.8824\n",
      "Epoch 03 | Training Loss: 0.5285 | Val Loss: 0.7956 | Accuracy: 0.8618 | Precision: 0.8083 | Recall: 0.9373\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.7898 | Test Accuracy: 0.8595 | Test Precision: 0.8157 | Test Recall: 0.9289\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 2/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 1 | Hidden Sizes: [128] | Batch Size: 128 | Optimizer: SGD | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.7807 | Val Loss: 0.7620 | Accuracy: 0.5352 | Precision: 0.5268 | Recall: 0.4059\n",
      "Epoch 02 | Training Loss: 0.7749 | Val Loss: 0.7584 | Accuracy: 0.5396 | Precision: 0.5308 | Recall: 0.4336\n",
      "Epoch 03 | Training Loss: 0.7709 | Val Loss: 0.7560 | Accuracy: 0.5372 | Precision: 0.5264 | Recall: 0.4526\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.7653 | Test Accuracy: 0.5290 | Test Precision: 0.5340 | Test Recall: 0.4549\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 3/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 3 | Hidden Sizes: [512, 512, 512] | Batch Size: 64 | Optimizer: Adam | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 1.1714 | Val Loss: 0.4785 | Accuracy: 0.8352 | Precision: 0.9167 | Recall: 0.7261\n",
      "Epoch 02 | Training Loss: 0.2979 | Val Loss: 0.4811 | Accuracy: 0.8594 | Precision: 0.8198 | Recall: 0.9101\n",
      "Epoch 03 | Training Loss: 0.1903 | Val Loss: 0.4538 | Accuracy: 0.8614 | Precision: 0.8915 | Recall: 0.8131\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.4772 | Test Accuracy: 0.8495 | Test Precision: 0.8949 | Test Recall: 0.7920\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 4/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 1 | Hidden Sizes: [512] | Batch Size: 128 | Optimizer: Adam | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6663 | Val Loss: 0.4707 | Accuracy: 0.7848 | Precision: 0.7868 | Recall: 0.7628\n",
      "Epoch 02 | Training Loss: 0.3659 | Val Loss: 0.3661 | Accuracy: 0.8410 | Precision: 0.8200 | Recall: 0.8610\n",
      "Epoch 03 | Training Loss: 0.2830 | Val Loss: 0.3321 | Accuracy: 0.8664 | Precision: 0.8520 | Recall: 0.8767\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3275 | Test Accuracy: 0.8640 | Test Precision: 0.8609 | Test Recall: 0.8682\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 5/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 1 | Hidden Sizes: [512] | Batch Size: 64 | Optimizer: SGD | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 1.0288 | Val Loss: 1.0074 | Accuracy: 0.5034 | Precision: 0.4884 | Recall: 0.5116\n",
      "Epoch 02 | Training Loss: 1.0104 | Val Loss: 0.9923 | Accuracy: 0.5138 | Precision: 0.4986 | Recall: 0.5025\n",
      "Epoch 03 | Training Loss: 0.9964 | Val Loss: 0.9794 | Accuracy: 0.5156 | Precision: 0.5004 | Recall: 0.5008\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.9682 | Test Accuracy: 0.5216 | Test Precision: 0.5217 | Test Recall: 0.5193\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 6/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 1 | Hidden Sizes: [512] | Batch Size: 32 | Optimizer: SGD | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.9504 | Val Loss: 0.8680 | Accuracy: 0.5702 | Precision: 0.5551 | Recall: 0.5718\n",
      "Epoch 02 | Training Loss: 0.8213 | Val Loss: 0.7944 | Accuracy: 0.5982 | Precision: 0.5797 | Recall: 0.6229\n",
      "Epoch 03 | Training Loss: 0.7523 | Val Loss: 0.7373 | Accuracy: 0.6256 | Precision: 0.6097 | Recall: 0.6328\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.7396 | Test Accuracy: 0.6331 | Test Precision: 0.6325 | Test Recall: 0.6356\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 7/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 1 | Hidden Sizes: [512] | Batch Size: 32 | Optimizer: Adam | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.5327 | Val Loss: 0.3661 | Accuracy: 0.8496 | Precision: 0.8460 | Recall: 0.8432\n",
      "Epoch 02 | Training Loss: 0.3240 | Val Loss: 0.3446 | Accuracy: 0.8746 | Precision: 0.8587 | Recall: 0.8874\n",
      "Epoch 03 | Training Loss: 0.2960 | Val Loss: 0.3450 | Accuracy: 0.8786 | Precision: 0.8731 | Recall: 0.8771\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3456 | Test Accuracy: 0.8756 | Test Precision: 0.8792 | Test Recall: 0.8708\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 8/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 3 | Hidden Sizes: [256, 256, 256] | Batch Size: 128 | Optimizer: SGD | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.7803 | Val Loss: 0.7570 | Accuracy: 0.5508 | Precision: 0.5396 | Recall: 0.5000\n",
      "Epoch 02 | Training Loss: 0.7404 | Val Loss: 0.7287 | Accuracy: 0.5676 | Precision: 0.5532 | Recall: 0.5623\n",
      "Epoch 03 | Training Loss: 0.7137 | Val Loss: 0.7107 | Accuracy: 0.5806 | Precision: 0.5626 | Recall: 0.6060\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.7097 | Test Accuracy: 0.5806 | Test Precision: 0.5771 | Test Recall: 0.6038\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 9/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 1 | Hidden Sizes: [512] | Batch Size: 64 | Optimizer: Adam | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4334 | Val Loss: 0.4199 | Accuracy: 0.8658 | Precision: 0.8107 | Recall: 0.9435\n",
      "Epoch 02 | Training Loss: 0.2622 | Val Loss: 0.4517 | Accuracy: 0.8732 | Precision: 0.8187 | Recall: 0.9484\n",
      "Epoch 03 | Training Loss: 0.2244 | Val Loss: 0.3776 | Accuracy: 0.8842 | Precision: 0.8730 | Recall: 0.8907\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3775 | Test Accuracy: 0.8830 | Test Precision: 0.8846 | Test Recall: 0.8810\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 10/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 1 | Hidden Sizes: [128] | Batch Size: 32 | Optimizer: SGD | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.8094 | Val Loss: 0.7963 | Accuracy: 0.5116 | Precision: 0.4963 | Recall: 0.4988\n",
      "Epoch 02 | Training Loss: 0.7999 | Val Loss: 0.7875 | Accuracy: 0.5158 | Precision: 0.5006 | Recall: 0.4996\n",
      "Epoch 03 | Training Loss: 0.7912 | Val Loss: 0.7797 | Accuracy: 0.5210 | Precision: 0.5060 | Recall: 0.5083\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.7883 | Test Accuracy: 0.5116 | Test Precision: 0.5117 | Test Recall: 0.5062\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 11/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 1 | Hidden Sizes: [512] | Batch Size: 64 | Optimizer: SGD | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 1.0864 | Val Loss: 1.0655 | Accuracy: 0.4914 | Precision: 0.4781 | Recall: 0.5355\n",
      "Epoch 02 | Training Loss: 1.0545 | Val Loss: 1.0452 | Accuracy: 0.4966 | Precision: 0.4823 | Recall: 0.5219\n",
      "Epoch 03 | Training Loss: 1.0366 | Val Loss: 1.0287 | Accuracy: 0.5026 | Precision: 0.4880 | Recall: 0.5268\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 1.0196 | Test Accuracy: 0.5031 | Test Precision: 0.5030 | Test Recall: 0.5264\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 12/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 2 | Hidden Sizes: [512, 512] | Batch Size: 32 | Optimizer: Adam | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4353 | Val Loss: 0.3317 | Accuracy: 0.8666 | Precision: 0.8535 | Recall: 0.8750\n",
      "Epoch 02 | Training Loss: 0.2278 | Val Loss: 0.3273 | Accuracy: 0.8740 | Precision: 0.8903 | Recall: 0.8441\n",
      "Epoch 03 | Training Loss: 0.1487 | Val Loss: 0.3315 | Accuracy: 0.8710 | Precision: 0.8674 | Recall: 0.8663\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3363 | Test Accuracy: 0.8670 | Test Precision: 0.8748 | Test Recall: 0.8566\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 13/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 2 | Hidden Sizes: [128, 128] | Batch Size: 64 | Optimizer: Adam | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.3833 | Val Loss: 0.3736 | Accuracy: 0.8656 | Precision: 0.8037 | Recall: 0.9563\n",
      "Epoch 02 | Training Loss: 0.2658 | Val Loss: 0.3100 | Accuracy: 0.8792 | Precision: 0.9059 | Recall: 0.8379\n",
      "Epoch 03 | Training Loss: 0.2410 | Val Loss: 0.3064 | Accuracy: 0.8840 | Precision: 0.8954 | Recall: 0.8614\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3157 | Test Accuracy: 0.8826 | Test Precision: 0.9081 | Test Recall: 0.8513\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 14/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 2 | Hidden Sizes: [256, 256] | Batch Size: 64 | Optimizer: RMSprop | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4114 | Val Loss: 0.3231 | Accuracy: 0.8638 | Precision: 0.8666 | Recall: 0.8498\n",
      "Epoch 02 | Training Loss: 0.2224 | Val Loss: 0.3260 | Accuracy: 0.8640 | Precision: 0.9041 | Recall: 0.8049\n",
      "Epoch 03 | Training Loss: 0.1520 | Val Loss: 0.3450 | Accuracy: 0.8694 | Precision: 0.8444 | Recall: 0.8956\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3354 | Test Accuracy: 0.8750 | Test Precision: 0.8624 | Test Recall: 0.8925\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 15/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 1 | Hidden Sizes: [256] | Batch Size: 128 | Optimizer: Adam | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4169 | Val Loss: 0.2895 | Accuracy: 0.8744 | Precision: 0.8583 | Recall: 0.8874\n",
      "Epoch 02 | Training Loss: 0.2370 | Val Loss: 0.2810 | Accuracy: 0.8876 | Precision: 0.8942 | Recall: 0.8713\n",
      "Epoch 03 | Training Loss: 0.2032 | Val Loss: 0.2876 | Accuracy: 0.8898 | Precision: 0.8665 | Recall: 0.9134\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3001 | Test Accuracy: 0.8852 | Test Precision: 0.8727 | Test Recall: 0.9019\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 16/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 3 | Hidden Sizes: [256, 256, 256] | Batch Size: 64 | Optimizer: SGD | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.8645 | Val Loss: 0.8695 | Accuracy: 0.5136 | Precision: 0.4984 | Recall: 0.5268\n",
      "Epoch 02 | Training Loss: 0.8478 | Val Loss: 0.8536 | Accuracy: 0.5164 | Precision: 0.5011 | Recall: 0.5417\n",
      "Epoch 03 | Training Loss: 0.8327 | Val Loss: 0.8396 | Accuracy: 0.5218 | Precision: 0.5062 | Recall: 0.5590\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.8319 | Test Accuracy: 0.5190 | Test Precision: 0.5178 | Test Recall: 0.5515\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 17/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 2 | Hidden Sizes: [256, 256] | Batch Size: 32 | Optimizer: RMSprop | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4948 | Val Loss: 0.3717 | Accuracy: 0.8612 | Precision: 0.8979 | Recall: 0.8053\n",
      "Epoch 02 | Training Loss: 0.2757 | Val Loss: 0.5278 | Accuracy: 0.8680 | Precision: 0.8212 | Recall: 0.9303\n",
      "Epoch 03 | Training Loss: 0.1242 | Val Loss: 0.6356 | Accuracy: 0.8698 | Precision: 0.8880 | Recall: 0.8370\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.6305 | Test Accuracy: 0.8577 | Test Precision: 0.8904 | Test Recall: 0.8158\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 18/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 2 | Hidden Sizes: [512, 512] | Batch Size: 64 | Optimizer: RMSprop | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.7911 | Val Loss: 0.3749 | Accuracy: 0.8586 | Precision: 0.8339 | Recall: 0.8845\n",
      "Epoch 02 | Training Loss: 0.2173 | Val Loss: 0.5452 | Accuracy: 0.8222 | Precision: 0.7459 | Recall: 0.9604\n",
      "Epoch 03 | Training Loss: 0.0784 | Val Loss: 0.5604 | Accuracy: 0.8588 | Precision: 0.8276 | Recall: 0.8952\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.5502 | Test Accuracy: 0.8596 | Test Precision: 0.8362 | Test Recall: 0.8942\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 19/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 1 | Hidden Sizes: [512] | Batch Size: 128 | Optimizer: Adam | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4546 | Val Loss: 0.3348 | Accuracy: 0.8762 | Precision: 0.8494 | Recall: 0.9051\n",
      "Epoch 02 | Training Loss: 0.2519 | Val Loss: 0.3372 | Accuracy: 0.8838 | Precision: 0.8903 | Recall: 0.8672\n",
      "Epoch 03 | Training Loss: 0.2120 | Val Loss: 0.3462 | Accuracy: 0.8866 | Precision: 0.8733 | Recall: 0.8960\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3554 | Test Accuracy: 0.8809 | Test Precision: 0.8802 | Test Recall: 0.8818\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 20/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 1 | Hidden Sizes: [512] | Batch Size: 64 | Optimizer: SGD | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 1.0424 | Val Loss: 0.9925 | Accuracy: 0.5112 | Precision: 0.4961 | Recall: 0.5182\n",
      "Epoch 02 | Training Loss: 0.9656 | Val Loss: 0.9261 | Accuracy: 0.5340 | Precision: 0.5186 | Recall: 0.5404\n",
      "Epoch 03 | Training Loss: 0.9043 | Val Loss: 0.8753 | Accuracy: 0.5546 | Precision: 0.5374 | Recall: 0.5842\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.9076 | Test Accuracy: 0.5610 | Test Precision: 0.5601 | Test Recall: 0.5685\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 21/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 1 | Hidden Sizes: [256] | Batch Size: 32 | Optimizer: Adam | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.3537 | Val Loss: 0.2899 | Accuracy: 0.8836 | Precision: 0.8678 | Recall: 0.8965\n",
      "Epoch 02 | Training Loss: 0.2535 | Val Loss: 0.2854 | Accuracy: 0.8838 | Precision: 0.8860 | Recall: 0.8725\n",
      "Epoch 03 | Training Loss: 0.2391 | Val Loss: 0.2864 | Accuracy: 0.8832 | Precision: 0.8625 | Recall: 0.9031\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.2835 | Test Accuracy: 0.8856 | Test Precision: 0.8778 | Test Recall: 0.8960\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 22/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 1 | Hidden Sizes: [512] | Batch Size: 32 | Optimizer: RMSprop | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4528 | Val Loss: 0.3486 | Accuracy: 0.8588 | Precision: 0.9162 | Recall: 0.7801\n",
      "Epoch 02 | Training Loss: 0.2553 | Val Loss: 0.3861 | Accuracy: 0.8568 | Precision: 0.7969 | Recall: 0.9455\n",
      "Epoch 03 | Training Loss: 0.1967 | Val Loss: 0.3679 | Accuracy: 0.8710 | Precision: 0.8358 | Recall: 0.9134\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3770 | Test Accuracy: 0.8674 | Test Precision: 0.8449 | Test Recall: 0.9000\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 23/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 2 | Hidden Sizes: [128, 128] | Batch Size: 128 | Optimizer: SGD | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.7392 | Val Loss: 0.7226 | Accuracy: 0.5388 | Precision: 0.5230 | Recall: 0.5545\n",
      "Epoch 02 | Training Loss: 0.7199 | Val Loss: 0.7146 | Accuracy: 0.5440 | Precision: 0.5311 | Recall: 0.5066\n",
      "Epoch 03 | Training Loss: 0.7142 | Val Loss: 0.7098 | Accuracy: 0.5490 | Precision: 0.5359 | Recall: 0.5206\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.7158 | Test Accuracy: 0.5446 | Test Precision: 0.5471 | Test Recall: 0.5177\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 24/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 2 | Hidden Sizes: [128, 128] | Batch Size: 32 | Optimizer: SGD | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.7902 | Val Loss: 0.7341 | Accuracy: 0.5304 | Precision: 0.5099 | Recall: 0.8094\n",
      "Epoch 02 | Training Loss: 0.7120 | Val Loss: 0.7074 | Accuracy: 0.5450 | Precision: 0.5251 | Recall: 0.6432\n",
      "Epoch 03 | Training Loss: 0.7015 | Val Loss: 0.7018 | Accuracy: 0.5490 | Precision: 0.5321 | Recall: 0.5784\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.6950 | Test Accuracy: 0.5612 | Test Precision: 0.5581 | Test Recall: 0.5881\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 25/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 2 | Hidden Sizes: [512, 512] | Batch Size: 32 | Optimizer: SGD | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 1.1404 | Val Loss: 1.1199 | Accuracy: 0.5438 | Precision: 0.5304 | Recall: 0.5153\n",
      "Epoch 02 | Training Loss: 1.0645 | Val Loss: 1.0575 | Accuracy: 0.5586 | Precision: 0.5466 | Recall: 0.5252\n",
      "Epoch 03 | Training Loss: 1.0049 | Val Loss: 1.0064 | Accuracy: 0.5672 | Precision: 0.5565 | Recall: 0.5281\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.9656 | Test Accuracy: 0.5736 | Test Precision: 0.5781 | Test Recall: 0.5449\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 26/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 3 | Hidden Sizes: [256, 256, 256] | Batch Size: 32 | Optimizer: RMSprop | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.5182 | Val Loss: 0.4346 | Accuracy: 0.8286 | Precision: 0.7766 | Recall: 0.9076\n",
      "Epoch 02 | Training Loss: 0.3188 | Val Loss: 0.4461 | Accuracy: 0.8276 | Precision: 0.9009 | Recall: 0.7240\n",
      "Epoch 03 | Training Loss: 0.2434 | Val Loss: 0.4163 | Accuracy: 0.8616 | Precision: 0.8268 | Recall: 0.9039\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.4163 | Test Accuracy: 0.8546 | Test Precision: 0.8342 | Test Recall: 0.8851\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 27/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 1 | Hidden Sizes: [256] | Batch Size: 128 | Optimizer: Adam | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4186 | Val Loss: 0.3041 | Accuracy: 0.8720 | Precision: 0.8732 | Recall: 0.8610\n",
      "Epoch 02 | Training Loss: 0.2379 | Val Loss: 0.3024 | Accuracy: 0.8750 | Precision: 0.8992 | Recall: 0.8358\n",
      "Epoch 03 | Training Loss: 0.2040 | Val Loss: 0.2931 | Accuracy: 0.8878 | Precision: 0.8854 | Recall: 0.8828\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3034 | Test Accuracy: 0.8833 | Test Precision: 0.8965 | Test Recall: 0.8666\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 28/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 2 | Hidden Sizes: [256, 256] | Batch Size: 32 | Optimizer: Adam | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.5997 | Val Loss: 0.4714 | Accuracy: 0.7808 | Precision: 0.7662 | Recall: 0.7884\n",
      "Epoch 02 | Training Loss: 0.3930 | Val Loss: 0.4178 | Accuracy: 0.8344 | Precision: 0.7949 | Recall: 0.8874\n",
      "Epoch 03 | Training Loss: 0.3319 | Val Loss: 0.3950 | Accuracy: 0.8536 | Precision: 0.8236 | Recall: 0.8882\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3904 | Test Accuracy: 0.8547 | Test Precision: 0.8376 | Test Recall: 0.8801\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 29/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 1 | Hidden Sizes: [128] | Batch Size: 128 | Optimizer: Adam | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.7244 | Val Loss: 0.5890 | Accuracy: 0.6880 | Precision: 0.6803 | Recall: 0.6724\n",
      "Epoch 02 | Training Loss: 0.5046 | Val Loss: 0.4558 | Accuracy: 0.7918 | Precision: 0.7783 | Recall: 0.7979\n",
      "Epoch 03 | Training Loss: 0.3951 | Val Loss: 0.3860 | Accuracy: 0.8312 | Precision: 0.8165 | Recall: 0.8408\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3946 | Test Accuracy: 0.8262 | Test Precision: 0.8255 | Test Recall: 0.8274\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 30/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 1 | Hidden Sizes: [128] | Batch Size: 128 | Optimizer: RMSprop | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4356 | Val Loss: 0.3671 | Accuracy: 0.8484 | Precision: 0.7859 | Recall: 0.9447\n",
      "Epoch 02 | Training Loss: 0.2591 | Val Loss: 0.2951 | Accuracy: 0.8792 | Precision: 0.8745 | Recall: 0.8767\n",
      "Epoch 03 | Training Loss: 0.1950 | Val Loss: 0.3052 | Accuracy: 0.8810 | Precision: 0.8508 | Recall: 0.9150\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3137 | Test Accuracy: 0.8722 | Test Precision: 0.8558 | Test Recall: 0.8951\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 31/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 1 | Hidden Sizes: [128] | Batch Size: 32 | Optimizer: Adam | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.5635 | Val Loss: 0.4325 | Accuracy: 0.8082 | Precision: 0.7887 | Recall: 0.8255\n",
      "Epoch 02 | Training Loss: 0.3596 | Val Loss: 0.3428 | Accuracy: 0.8528 | Precision: 0.8360 | Recall: 0.8663\n",
      "Epoch 03 | Training Loss: 0.2981 | Val Loss: 0.3190 | Accuracy: 0.8660 | Precision: 0.8533 | Recall: 0.8738\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3212 | Test Accuracy: 0.8645 | Test Precision: 0.8700 | Test Recall: 0.8571\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 32/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 3 | Hidden Sizes: [256, 256, 256] | Batch Size: 32 | Optimizer: RMSprop | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4076 | Val Loss: 0.3688 | Accuracy: 0.8486 | Precision: 0.7840 | Recall: 0.9493\n",
      "Epoch 02 | Training Loss: 0.2279 | Val Loss: 0.3118 | Accuracy: 0.8752 | Precision: 0.8889 | Recall: 0.8486\n",
      "Epoch 03 | Training Loss: 0.1597 | Val Loss: 0.3284 | Accuracy: 0.8796 | Precision: 0.8857 | Recall: 0.8630\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3497 | Test Accuracy: 0.8734 | Test Precision: 0.8869 | Test Recall: 0.8561\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 33/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 3 | Hidden Sizes: [512, 512, 512] | Batch Size: 64 | Optimizer: SGD | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 1.1054 | Val Loss: 1.0129 | Accuracy: 0.5298 | Precision: 0.5149 | Recall: 0.5215\n",
      "Epoch 02 | Training Loss: 0.9430 | Val Loss: 0.8956 | Accuracy: 0.5606 | Precision: 0.5446 | Recall: 0.5722\n",
      "Epoch 03 | Training Loss: 0.8431 | Val Loss: 0.8190 | Accuracy: 0.5884 | Precision: 0.5722 | Recall: 0.5982\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.8268 | Test Accuracy: 0.5909 | Test Precision: 0.5904 | Test Recall: 0.5939\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 34/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 3 | Hidden Sizes: [128, 128, 128] | Batch Size: 128 | Optimizer: Adam | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6154 | Val Loss: 0.5254 | Accuracy: 0.7422 | Precision: 0.7389 | Recall: 0.7240\n",
      "Epoch 02 | Training Loss: 0.4456 | Val Loss: 0.4235 | Accuracy: 0.8078 | Precision: 0.7785 | Recall: 0.8436\n",
      "Epoch 03 | Training Loss: 0.3522 | Val Loss: 0.3673 | Accuracy: 0.8356 | Precision: 0.8181 | Recall: 0.8498\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3660 | Test Accuracy: 0.8377 | Test Precision: 0.8302 | Test Recall: 0.8490\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 35/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 2 | Hidden Sizes: [256, 256] | Batch Size: 128 | Optimizer: SGD | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.8981 | Val Loss: 0.8637 | Accuracy: 0.5138 | Precision: 0.4986 | Recall: 0.5239\n",
      "Epoch 02 | Training Loss: 0.8510 | Val Loss: 0.8300 | Accuracy: 0.5280 | Precision: 0.5129 | Recall: 0.5248\n",
      "Epoch 03 | Training Loss: 0.8194 | Val Loss: 0.8015 | Accuracy: 0.5404 | Precision: 0.5262 | Recall: 0.5219\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.8053 | Test Accuracy: 0.5394 | Test Precision: 0.5403 | Test Recall: 0.5273\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 36/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 3 | Hidden Sizes: [256, 256, 256] | Batch Size: 64 | Optimizer: Adam | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.3903 | Val Loss: 0.3217 | Accuracy: 0.8678 | Precision: 0.8244 | Recall: 0.9241\n",
      "Epoch 02 | Training Loss: 0.2180 | Val Loss: 0.3048 | Accuracy: 0.8862 | Precision: 0.8533 | Recall: 0.9241\n",
      "Epoch 03 | Training Loss: 0.1553 | Val Loss: 0.3064 | Accuracy: 0.8802 | Precision: 0.8826 | Recall: 0.8684\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3137 | Test Accuracy: 0.8787 | Test Precision: 0.8950 | Test Recall: 0.8582\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 37/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 3 | Hidden Sizes: [256, 256, 256] | Batch Size: 32 | Optimizer: SGD | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.8818 | Val Loss: 0.8559 | Accuracy: 0.5090 | Precision: 0.4941 | Recall: 0.5363\n",
      "Epoch 02 | Training Loss: 0.8405 | Val Loss: 0.8242 | Accuracy: 0.5184 | Precision: 0.5032 | Recall: 0.5206\n",
      "Epoch 03 | Training Loss: 0.8119 | Val Loss: 0.7988 | Accuracy: 0.5264 | Precision: 0.5112 | Recall: 0.5268\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.8108 | Test Accuracy: 0.5255 | Test Precision: 0.5253 | Test Recall: 0.5284\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 38/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 2 | Hidden Sizes: [128, 128] | Batch Size: 64 | Optimizer: SGD | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.7596 | Val Loss: 0.7424 | Accuracy: 0.4836 | Precision: 0.4683 | Recall: 0.4823\n",
      "Epoch 02 | Training Loss: 0.7267 | Val Loss: 0.7230 | Accuracy: 0.5042 | Precision: 0.4891 | Recall: 0.5111\n",
      "Epoch 03 | Training Loss: 0.7094 | Val Loss: 0.7082 | Accuracy: 0.5256 | Precision: 0.5109 | Recall: 0.5012\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.7032 | Test Accuracy: 0.5392 | Test Precision: 0.5409 | Test Recall: 0.5187\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 39/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 2 | Hidden Sizes: [256, 256] | Batch Size: 64 | Optimizer: SGD | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.8419 | Val Loss: 0.7709 | Accuracy: 0.5608 | Precision: 0.5457 | Recall: 0.5611\n",
      "Epoch 02 | Training Loss: 0.7476 | Val Loss: 0.7075 | Accuracy: 0.5982 | Precision: 0.5897 | Recall: 0.5627\n",
      "Epoch 03 | Training Loss: 0.6943 | Val Loss: 0.6709 | Accuracy: 0.6242 | Precision: 0.6040 | Recall: 0.6526\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.6894 | Test Accuracy: 0.6144 | Test Precision: 0.6086 | Test Recall: 0.6410\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 40/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 1 | Hidden Sizes: [256] | Batch Size: 128 | Optimizer: SGD | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.8926 | Val Loss: 0.8706 | Accuracy: 0.4810 | Precision: 0.4685 | Recall: 0.5252\n",
      "Epoch 02 | Training Loss: 0.8687 | Val Loss: 0.8524 | Accuracy: 0.4908 | Precision: 0.4766 | Recall: 0.5124\n",
      "Epoch 03 | Training Loss: 0.8517 | Val Loss: 0.8370 | Accuracy: 0.4966 | Precision: 0.4821 | Recall: 0.5182\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.8306 | Test Accuracy: 0.5099 | Test Precision: 0.5093 | Test Recall: 0.5394\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 41/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 3 | Hidden Sizes: [256, 256, 256] | Batch Size: 128 | Optimizer: RMSprop | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4766 | Val Loss: 0.3375 | Accuracy: 0.8560 | Precision: 0.8444 | Recall: 0.8618\n",
      "Epoch 02 | Training Loss: 0.2324 | Val Loss: 0.3375 | Accuracy: 0.8568 | Precision: 0.9013 | Recall: 0.7913\n",
      "Epoch 03 | Training Loss: 0.1430 | Val Loss: 0.3337 | Accuracy: 0.8776 | Precision: 0.8592 | Recall: 0.8940\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3430 | Test Accuracy: 0.8717 | Test Precision: 0.8643 | Test Recall: 0.8818\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 42/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 1 | Hidden Sizes: [512] | Batch Size: 128 | Optimizer: SGD | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 1.0961 | Val Loss: 1.0624 | Accuracy: 0.5060 | Precision: 0.4908 | Recall: 0.5062\n",
      "Epoch 02 | Training Loss: 1.0088 | Val Loss: 0.9913 | Accuracy: 0.5272 | Precision: 0.5120 | Recall: 0.5260\n",
      "Epoch 03 | Training Loss: 0.9422 | Val Loss: 0.9325 | Accuracy: 0.5488 | Precision: 0.5342 | Recall: 0.5408\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.9031 | Test Accuracy: 0.5606 | Test Precision: 0.5621 | Test Recall: 0.5486\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 43/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 3 | Hidden Sizes: [128, 128, 128] | Batch Size: 128 | Optimizer: SGD | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.7846 | Val Loss: 0.7633 | Accuracy: 0.4806 | Precision: 0.4565 | Recall: 0.3746\n",
      "Epoch 02 | Training Loss: 0.7549 | Val Loss: 0.7540 | Accuracy: 0.4822 | Precision: 0.4640 | Recall: 0.4385\n",
      "Epoch 03 | Training Loss: 0.7457 | Val Loss: 0.7460 | Accuracy: 0.4868 | Precision: 0.4692 | Recall: 0.4464\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.7413 | Test Accuracy: 0.4878 | Test Precision: 0.4870 | Test Recall: 0.4566\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 44/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 1 | Hidden Sizes: [256] | Batch Size: 64 | Optimizer: Adam | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4056 | Val Loss: 0.3387 | Accuracy: 0.8812 | Precision: 0.8781 | Recall: 0.8767\n",
      "Epoch 02 | Training Loss: 0.2701 | Val Loss: 0.3505 | Accuracy: 0.8792 | Precision: 0.8476 | Recall: 0.9154\n",
      "Epoch 03 | Training Loss: 0.2413 | Val Loss: 0.3777 | Accuracy: 0.8744 | Precision: 0.8304 | Recall: 0.9311\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3642 | Test Accuracy: 0.8780 | Test Precision: 0.8433 | Test Recall: 0.9285\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 45/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 2 | Hidden Sizes: [256, 256] | Batch Size: 64 | Optimizer: SGD | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 1.2233 | Val Loss: 0.9627 | Accuracy: 0.5050 | Precision: 0.4848 | Recall: 0.3366\n",
      "Epoch 02 | Training Loss: 0.9328 | Val Loss: 0.9257 | Accuracy: 0.4984 | Precision: 0.4813 | Recall: 0.4468\n",
      "Epoch 03 | Training Loss: 0.9064 | Val Loss: 0.9076 | Accuracy: 0.5010 | Precision: 0.4849 | Recall: 0.4699\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.8938 | Test Accuracy: 0.5106 | Test Precision: 0.5113 | Test Recall: 0.4821\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 46/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 3 | Hidden Sizes: [256, 256, 256] | Batch Size: 128 | Optimizer: SGD | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.9325 | Val Loss: 0.9078 | Accuracy: 0.5142 | Precision: 0.4990 | Recall: 0.5111\n",
      "Epoch 02 | Training Loss: 0.9122 | Val Loss: 0.8917 | Accuracy: 0.5164 | Precision: 0.5012 | Recall: 0.4963\n",
      "Epoch 03 | Training Loss: 0.8974 | Val Loss: 0.8780 | Accuracy: 0.5232 | Precision: 0.5084 | Recall: 0.4996\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.8847 | Test Accuracy: 0.5192 | Test Precision: 0.5199 | Test Recall: 0.5003\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 47/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 3 | Hidden Sizes: [512, 512, 512] | Batch Size: 64 | Optimizer: Adam | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.9800 | Val Loss: 0.3800 | Accuracy: 0.8694 | Precision: 0.8828 | Recall: 0.8424\n",
      "Epoch 02 | Training Loss: 0.2834 | Val Loss: 0.5604 | Accuracy: 0.8354 | Precision: 0.7619 | Recall: 0.9608\n",
      "Epoch 03 | Training Loss: 0.1872 | Val Loss: 0.5002 | Accuracy: 0.8638 | Precision: 0.8219 | Recall: 0.9179\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.5144 | Test Accuracy: 0.8603 | Test Precision: 0.8311 | Test Recall: 0.9044\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 48/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 3 | Hidden Sizes: [128, 128, 128] | Batch Size: 64 | Optimizer: RMSprop | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4917 | Val Loss: 0.3920 | Accuracy: 0.8262 | Precision: 0.7900 | Recall: 0.8738\n",
      "Epoch 02 | Training Loss: 0.3161 | Val Loss: 0.3346 | Accuracy: 0.8548 | Precision: 0.8437 | Recall: 0.8597\n",
      "Epoch 03 | Training Loss: 0.2573 | Val Loss: 0.3177 | Accuracy: 0.8664 | Precision: 0.8558 | Recall: 0.8713\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3214 | Test Accuracy: 0.8642 | Test Precision: 0.8653 | Test Recall: 0.8628\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 49/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 2 | Hidden Sizes: [256, 256] | Batch Size: 32 | Optimizer: RMSprop | Activation Function: leaky_relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4440 | Val Loss: 0.3132 | Accuracy: 0.8758 | Precision: 0.8867 | Recall: 0.8527\n",
      "Epoch 02 | Training Loss: 0.2667 | Val Loss: 0.3186 | Accuracy: 0.8790 | Precision: 0.8977 | Recall: 0.8469\n",
      "Epoch 03 | Training Loss: 0.2394 | Val Loss: 0.3615 | Accuracy: 0.8834 | Precision: 0.8689 | Recall: 0.8944\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3859 | Test Accuracy: 0.8770 | Test Precision: 0.8742 | Test Recall: 0.8808\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Run 50/50\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0001 | Num Layers: 3 | Hidden Sizes: [128, 128, 128] | Batch Size: 32 | Optimizer: SGD | Activation Function: tanh\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.7781 | Val Loss: 0.7677 | Accuracy: 0.4828 | Precision: 0.4698 | Recall: 0.5194\n",
      "Epoch 02 | Training Loss: 0.7665 | Val Loss: 0.7568 | Accuracy: 0.4878 | Precision: 0.4732 | Recall: 0.4992\n",
      "Epoch 03 | Training Loss: 0.7571 | Val Loss: 0.7479 | Accuracy: 0.4936 | Precision: 0.4784 | Recall: 0.4926\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.7551 | Test Accuracy: 0.4906 | Test Precision: 0.4907 | Test Recall: 0.4912\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Best performing model:\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0010 | Num Layers: 1 | Hidden Sizes: [256] | Batch Size: 32 | Optimizer: Adam | Activation Function: tanh\n",
      "\n",
      "Performance Metrics:\n",
      "Loss: 0.2835 | Accuracy: 0.8856 | Precision: 0.8778 | Recall: 0.8960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducibility.\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "# -----------\n",
    "# Model Setup\n",
    "# -----------\n",
    "# The input size is determined by the dimension of the bag-of-characters vector.\n",
    "size_input = X_train_word.shape[1]\n",
    "size_output  = 2 # Binary Classification\n",
    "results = [] # Will contain the results of each run.\n",
    "\n",
    "combinations = 50 # Number of random hyperparameter combinations to try.\n",
    "\n",
    "for i in range(combinations):\n",
    "    # Gets a new set of random parameters.\n",
    "    params = Hyperparameters()\n",
    "    params.sample_random()\n",
    "    \n",
    "    # Logs the parameters\n",
    "    print(f\"\\nRun {i+1}/{combinations}\")\n",
    "    print(params)\n",
    "\n",
    "    # Creates a new model with the random parameters.\n",
    "    model = MLP(size_input = size_input,\n",
    "                num_hidden = params.hidden_layers,\n",
    "                hidden_sizes = params.hidden_sizes,\n",
    "                size_output = size_output,\n",
    "                batch_size = params.batch_size,\n",
    "                optimizer = params.optimizer,\n",
    "                learning_rate = params.learning_rate,\n",
    "                act_func = params.activation_function,\n",
    "                device=None)\n",
    "\n",
    "    # Trains the model, tests it, and logs the results.\n",
    "    train_model(model, X_train_word, X_val_word, y_train, y_val, epochs=3)\n",
    "    pm = PerformanceMetrics(*test_model(model, X_test_word, y_test))\n",
    "    results.append(Result(params, pm))\n",
    "\n",
    "    print(\"\\n-----------------------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "# Displays the best performing model (based on accuracy).\n",
    "best_result = max(results, key=lambda r: r.performance_metrics.accuracy)\n",
    "\n",
    "print(\"\\nBest performing model:\")\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Robustness of Model with Best Parameters\n",
    "\n",
    "So using the technique above for hyper-parameter optimization, I have found that the following are the best parameters for the word-tokenization model:\n",
    "- **Learning Rate**: `0.001`\n",
    "- **Number of Layers**: `1`\n",
    "- **Hidden Layer Size**: `256`\n",
    "- **Batch Size**: `32`\n",
    "- **Optimizer**: `Adam`\n",
    "- **Activation Function**: `Tanh`\n",
    "\n",
    "In order to further confirm this however, a final robustness check can be done.\n",
    "\n",
    "The following code below runs the model with the best found parameters multiple times, and reports the mean accuracy and standard error across all runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment Run 1/5\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.3613 | Val Loss: 0.2988 | Accuracy: 0.8766 | Precision: 0.8944 | Recall: 0.8453\n",
      "Epoch 02 | Training Loss: 0.2540 | Val Loss: 0.2844 | Accuracy: 0.8860 | Precision: 0.8768 | Recall: 0.8899\n",
      "Epoch 03 | Training Loss: 0.2398 | Val Loss: 0.2890 | Accuracy: 0.8816 | Precision: 0.8494 | Recall: 0.9187\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.2832 | Test Accuracy: 0.8840 | Test Precision: 0.8645 | Test Recall: 0.9106\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Experiment Run 2/5\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.3575 | Val Loss: 0.2950 | Accuracy: 0.8802 | Precision: 0.8591 | Recall: 0.9006\n",
      "Epoch 02 | Training Loss: 0.2551 | Val Loss: 0.2900 | Accuracy: 0.8788 | Precision: 0.8451 | Recall: 0.9183\n",
      "Epoch 03 | Training Loss: 0.2411 | Val Loss: 0.2801 | Accuracy: 0.8858 | Precision: 0.8840 | Recall: 0.8800\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.2818 | Test Accuracy: 0.8823 | Test Precision: 0.8966 | Test Recall: 0.8643\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Experiment Run 3/5\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.3549 | Val Loss: 0.2920 | Accuracy: 0.8760 | Precision: 0.8838 | Recall: 0.8568\n",
      "Epoch 02 | Training Loss: 0.2539 | Val Loss: 0.2834 | Accuracy: 0.8858 | Precision: 0.8623 | Recall: 0.9097\n",
      "Epoch 03 | Training Loss: 0.2392 | Val Loss: 0.2836 | Accuracy: 0.8830 | Precision: 0.8823 | Recall: 0.8754\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.2862 | Test Accuracy: 0.8846 | Test Precision: 0.8960 | Test Recall: 0.8702\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Experiment Run 4/5\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.3579 | Val Loss: 0.2997 | Accuracy: 0.8786 | Precision: 0.8644 | Recall: 0.8890\n",
      "Epoch 02 | Training Loss: 0.2541 | Val Loss: 0.2884 | Accuracy: 0.8830 | Precision: 0.8579 | Recall: 0.9092\n",
      "Epoch 03 | Training Loss: 0.2399 | Val Loss: 0.2863 | Accuracy: 0.8862 | Precision: 0.8591 | Recall: 0.9154\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.2841 | Test Accuracy: 0.8870 | Test Precision: 0.8731 | Test Recall: 0.9056\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Experiment Run 5/5\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.3486 | Val Loss: 0.2937 | Accuracy: 0.8756 | Precision: 0.8818 | Recall: 0.8585\n",
      "Epoch 02 | Training Loss: 0.2533 | Val Loss: 0.2810 | Accuracy: 0.8840 | Precision: 0.8748 | Recall: 0.8878\n",
      "Epoch 03 | Training Loss: 0.2407 | Val Loss: 0.2830 | Accuracy: 0.8850 | Precision: 0.8588 | Recall: 0.9130\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.2809 | Test Accuracy: 0.8866 | Test Precision: 0.8729 | Test Recall: 0.9049\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Summary of Experiment Runs:\n",
      "Run 1: Accuracy = 0.8840\n",
      "Run 2: Accuracy = 0.8823\n",
      "Run 3: Accuracy = 0.8846\n",
      "Run 4: Accuracy = 0.8870\n",
      "Run 5: Accuracy = 0.8866\n",
      "\n",
      "Mean Accuracy: 0.8849\n",
      "Standard Error: 0.0008\n"
     ]
    }
   ],
   "source": [
    "# Ensures seeds are now random.\n",
    "tf.random.set_seed(None)\n",
    "np.random.seed(None)\n",
    "\n",
    "\n",
    "size_input = X_train_word.shape[1]\n",
    "\n",
    "# Best parameters found from random search.\n",
    "num_hidden = 1 # Number of hidden layers\n",
    "hidden_sizes = [256] # Size of each hidden layer\n",
    "batch_size = 32\n",
    "optimizer = tf.keras.optimizers.Adam\n",
    "learning_rate = 0.001\n",
    "act_func = tf.nn.tanh # Activation function used\n",
    "size_output  = 2 # Binary classification\n",
    "\n",
    "results = []\n",
    "experiment_runs = 5\n",
    "\n",
    "for i in range(experiment_runs):\n",
    "    print(f\"\\nExperiment Run {i+1}/{experiment_runs}\")\n",
    "    \n",
    "    model = MLP(size_input=size_input,\n",
    "                num_hidden=num_hidden,\n",
    "                hidden_sizes=hidden_sizes,\n",
    "                size_output=size_output,\n",
    "                batch_size=batch_size,\n",
    "                optimizer=optimizer,\n",
    "                learning_rate=learning_rate,\n",
    "                act_func=act_func,\n",
    "                device=None)\n",
    "    \n",
    "    #Trains the model, tests it, and logs the results.\n",
    "    train_model(model, X_train_word, X_val_word, y_train, y_val, epochs=3)\n",
    "    pm = PerformanceMetrics(*test_model(model, X_test_word, y_test))\n",
    "    results.append(Result(params, pm))\n",
    "\n",
    "    print(\"\\n-----------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "# Logs a summary of the accuracy from each run.\n",
    "print(\"\\nSummary of Experiment Runs:\")\n",
    "\n",
    "accuracies = [r.performance_metrics.accuracy for r in results]\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Run {i+1}: Accuracy = {accuracies[i]:.4f}\")\n",
    "\n",
    "# Computes mean accuracy and standard error.\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "standard_error = np.std(accuracies) / np.sqrt(len(accuracies))\n",
    "\n",
    "print(f\"\\nMean Accuracy: {mean_accuracy:.4f}\")\n",
    "print(f\"Standard Error: {standard_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Random Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Random MLP Class Definition\n",
    "# ---------------------------\n",
    "class MLP_rnd(object):\n",
    "    def __init__(self, size_input, num_hidden, hidden_sizes, size_output, batch_size, optimizer, learning_rate, act_func, device=None):\n",
    "        self.size_input = size_input # Number of features\n",
    "        self.num_hidden = num_hidden # Number of hidden layers\n",
    "        self.hidden_sizes = hidden_sizes # List of hidden layer sizes\n",
    "        self.size_output = size_output # Number of classes (Binary Classification)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer # Optimizer class (e.g. Adam, SGD, RMSprop)\n",
    "        self.act_func = act_func # Activation function to use (e.g. ReLU, Tanh, Leaky ReLU)\n",
    "        self.device = device # Device to run the model on (cpu or gpu)\n",
    "        \n",
    "        # Holds the model's weights and biases.\n",
    "        self.variables = []\n",
    "        \n",
    "        # Initializes the model's weights and biases.\n",
    "        w = tf.Variable(tf.random.normal([self.size_input, self.hidden_sizes[0]], stddev=0.1))\n",
    "        b = tf.Variable(tf.zeros([1, self.hidden_sizes[0]]))\n",
    "        self.variables.append((w, b))\n",
    "        \n",
    "        # Initializes the weights and biases for the hidden layers.\n",
    "        for i in range(1, self.num_hidden):\n",
    "            w = tf.Variable(tf.random.normal([self.hidden_sizes[i-1], self.hidden_sizes[i]], stddev=0.1))\n",
    "            b = tf.Variable(tf.zeros([1, self.hidden_sizes[i]]))\n",
    "            self.variables.append((w, b))\n",
    "        \n",
    "        # Initializes the weights and biases for the output layer.\n",
    "        w = tf.Variable(tf.random.normal([self.hidden_sizes[-1], self.size_output], stddev=0.1))\n",
    "        b = tf.Variable(tf.zeros([1, self.size_output]))\n",
    "        self.variables.append((w, b))\n",
    "        \n",
    "        # Output layer weights and bias used specifically for back propagation.\n",
    "        self.output_variables = [w, b]\n",
    "\n",
    "    # Computes the output tensor during the forward pass. X is the input tensor.\n",
    "    def forward(self, X):\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "        return self.y\n",
    "\n",
    "    # Computes the loss value given the predicted and actual output tensors.\n",
    "    def loss(self, y_pred, y_true):\n",
    "        # Tensor of shape (batch_size, size_output)\n",
    "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        loss_x = cce(y_true_tf, y_pred_tf)\n",
    "        return loss_x\n",
    "\n",
    "    # Computes and applies gradients of the loss with respect to the output variables (weights and bias).\n",
    "    def backward(self, X_train, y_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)  \n",
    "        \n",
    "        grads = tape.gradient(current_loss, self.output_variables)\n",
    "        \n",
    "        optimizer = self.optimizer(learning_rate=self.learning_rate)\n",
    "        optimizer.apply_gradients(zip(grads, self.output_variables))\n",
    "\n",
    "    # Custom method to compute the output tensor during the forward pass.\n",
    "    def compute_output(self, X):  \n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        z = X_tf\n",
    "        \n",
    "        # Goes through each hidden layer and applies the weights and biases\n",
    "        for w, b in self.variables:\n",
    "            z = tf.matmul(z, w) + b\n",
    "            \n",
    "            # If the current layer is the last layer, then its the output layer.\n",
    "            if (w, b) == self.variables[-1]:\n",
    "                return z\n",
    "            \n",
    "            z = self.act_func(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Random Model with Best Parameters\n",
    "\n",
    "Below, I now run the same set of experiements I did on best model, but with the random model this time (using the best parameters found through hyper-parameter optimization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment Run 1/5\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6670 | Val Loss: 0.5726 | Accuracy: 0.7024 | Precision: 0.6854 | Recall: 0.7137\n",
      "Epoch 02 | Training Loss: 0.5539 | Val Loss: 0.5541 | Accuracy: 0.7204 | Precision: 0.7137 | Recall: 0.7067\n",
      "Epoch 03 | Training Loss: 0.5447 | Val Loss: 0.5540 | Accuracy: 0.7130 | Precision: 0.7166 | Recall: 0.6749\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.5588 | Test Accuracy: 0.7168 | Test Precision: 0.7391 | Test Recall: 0.6702\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Experiment Run 2/5\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6439 | Val Loss: 0.5671 | Accuracy: 0.7094 | Precision: 0.6816 | Recall: 0.7517\n",
      "Epoch 02 | Training Loss: 0.5474 | Val Loss: 0.5501 | Accuracy: 0.7238 | Precision: 0.7272 | Recall: 0.6885\n",
      "Epoch 03 | Training Loss: 0.5390 | Val Loss: 0.5484 | Accuracy: 0.7228 | Precision: 0.7021 | Recall: 0.7438\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.5490 | Test Accuracy: 0.7214 | Test Precision: 0.7096 | Test Recall: 0.7494\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Experiment Run 3/5\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6476 | Val Loss: 0.5754 | Accuracy: 0.6956 | Precision: 0.6780 | Recall: 0.7087\n",
      "Epoch 02 | Training Loss: 0.5561 | Val Loss: 0.5493 | Accuracy: 0.7218 | Precision: 0.7130 | Recall: 0.7133\n",
      "Epoch 03 | Training Loss: 0.5457 | Val Loss: 0.5424 | Accuracy: 0.7238 | Precision: 0.7220 | Recall: 0.6997\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.5473 | Test Accuracy: 0.7210 | Test Precision: 0.7329 | Test Recall: 0.6956\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Experiment Run 4/5\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6473 | Val Loss: 0.5693 | Accuracy: 0.6996 | Precision: 0.6707 | Recall: 0.7471\n",
      "Epoch 02 | Training Loss: 0.5498 | Val Loss: 0.5488 | Accuracy: 0.7148 | Precision: 0.7252 | Recall: 0.6630\n",
      "Epoch 03 | Training Loss: 0.5394 | Val Loss: 0.5480 | Accuracy: 0.7204 | Precision: 0.7148 | Recall: 0.7042\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.5472 | Test Accuracy: 0.7199 | Test Precision: 0.7260 | Test Recall: 0.7065\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Experiment Run 5/5\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6519 | Val Loss: 0.5728 | Accuracy: 0.6960 | Precision: 0.6840 | Recall: 0.6931\n",
      "Epoch 02 | Training Loss: 0.5487 | Val Loss: 0.5589 | Accuracy: 0.7078 | Precision: 0.7038 | Recall: 0.6861\n",
      "Epoch 03 | Training Loss: 0.5393 | Val Loss: 0.5638 | Accuracy: 0.7086 | Precision: 0.6741 | Recall: 0.7723\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.5501 | Test Accuracy: 0.7211 | Test Precision: 0.7013 | Test Recall: 0.7703\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Summary of Experiment Runs:\n",
      "Run 1: Accuracy = 0.7168\n",
      "Run 2: Accuracy = 0.7214\n",
      "Run 3: Accuracy = 0.7210\n",
      "Run 4: Accuracy = 0.7199\n",
      "Run 5: Accuracy = 0.7211\n",
      "\n",
      "Mean Accuracy: 0.7201\n",
      "Standard Error: 0.0008\n"
     ]
    }
   ],
   "source": [
    "# Ensures seeds are now random again.\n",
    "tf.random.set_seed(None)\n",
    "np.random.seed(None)\n",
    "\n",
    "\n",
    "size_input = X_train_word.shape[1]\n",
    "\n",
    "# Best parameters found from random search.\n",
    "num_hidden = 1 # Number of hidden layers\n",
    "hidden_sizes = [256] # Size of each hidden layer\n",
    "batch_size = 32\n",
    "optimizer = tf.keras.optimizers.Adam\n",
    "learning_rate = 0.001\n",
    "act_func = tf.nn.tanh # Activation function used\n",
    "size_output  = 2 # Binary classification\n",
    "\n",
    "results = []\n",
    "experiment_runs = 5\n",
    "\n",
    "for i in range(experiment_runs):\n",
    "    print(f\"\\nExperiment Run {i+1}/{experiment_runs}\")\n",
    "    \n",
    "    model = MLP_rnd(size_input=size_input,\n",
    "                num_hidden=num_hidden,\n",
    "                hidden_sizes=hidden_sizes,\n",
    "                size_output=size_output,\n",
    "                batch_size=batch_size,\n",
    "                optimizer=optimizer,\n",
    "                learning_rate=learning_rate,\n",
    "                act_func=act_func,\n",
    "                device=None)\n",
    "    \n",
    "    #Trains the model, tests it, and logs the results.\n",
    "    train_model(model, X_train_word, X_val_word, y_train, y_val, epochs=3)\n",
    "    pm = PerformanceMetrics(*test_model(model, X_test_word, y_test))\n",
    "    results.append(Result(params, pm))\n",
    "\n",
    "    print(\"\\n-----------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "# Logs a summary of the accuracy from each run.\n",
    "print(\"\\nSummary of Experiment Runs:\")\n",
    "\n",
    "accuracies = [r.performance_metrics.accuracy for r in results]\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Run {i+1}: Accuracy = {accuracies[i]:.4f}\")\n",
    "\n",
    "# Computes mean accuracy and standard error.\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "standard_error = np.std(accuracies) / np.sqrt(len(accuracies))\n",
    "\n",
    "print(f\"\\nMean Accuracy: {mean_accuracy:.4f}\")\n",
    "print(f\"Standard Error: {standard_error:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01: [creative title name]\n",
    "\n",
    "### Maxim Ryabinov (U02204083)\n",
    "### CAP4641: Natural Language Processing \n",
    "### Instructor: Dr. Ankur Mali \n",
    "### University of South Florida (Spring 2025)\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Initial Setup\n",
    "- Sets random seeds to ensure reproducibility.\n",
    "- Creates \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Suppress TensorFlow warnings.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Data Storage Classes\n",
    "# --------------------\n",
    "class Hyperparameters():    \n",
    "    def __init__(self, learning_rate=None, hidden_layers=None, hidden_sizes=None, batch_size=None, optimizer=None, activation_function=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.activation_function = activation_function\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (f\"Learning Rate: {self.learning_rate:.4f} | Num Layers: {self.hidden_layers} | \"\n",
    "                f\"Hidden Sizes: {self.hidden_sizes} | Batch Size: {self.batch_size} | \"\n",
    "                f\"Optimizer: {self.optimizer.__name__} | Activation Function: {self.activation_function.__name__}\\n\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_hyperparameter_choices():\n",
    "        # Returns a dictionary of possible hyperparameter values.\n",
    "        return {\n",
    "            \"learning_rates\": [0.001, 0.0005, 0.0001],\n",
    "            \"hidden_layers\": [1, 2, 3],\n",
    "            \"hidden_sizes\": [128, 256, 512],\n",
    "            \"batch_sizes\": [32, 64, 128],\n",
    "            \"optimizers\": [tf.keras.optimizers.Adam, tf.keras.optimizers.SGD, tf.keras.optimizers.RMSprop],\n",
    "            \"activation_functions\": [tf.nn.relu, tf.nn.tanh, tf.nn.leaky_relu]\n",
    "        }\n",
    "        \n",
    "    def sample_random(self):        \n",
    "        choices = self.get_hyperparameter_choices()\n",
    "        self.learning_rate = random.choice(choices[\"learning_rates\"])\n",
    "        self.hidden_layers = random.choice(choices[\"hidden_layers\"])\n",
    "        picked_size = random.choice(choices[\"hidden_sizes\"])\n",
    "        self.hidden_sizes = [picked_size] * self.hidden_layers\n",
    "        self.batch_size = random.choice(choices[\"batch_sizes\"])\n",
    "        self.optimizer = random.choice(choices[\"optimizers\"])\n",
    "        self.activation_function = random.choice(choices[\"activation_functions\"])\n",
    "        \n",
    "        \n",
    "\n",
    "class PerformanceMetrics():\n",
    "    def __init__(self, loss, accuracy, precision, recall):\n",
    "        self.loss = loss\n",
    "        self.accuracy = accuracy\n",
    "        self.precision = precision\n",
    "        self.recall = recall\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (f\"Loss: {self.loss:.4f} | Accuracy: {self.accuracy:.4f} | \"\n",
    "                f\"Precision: {self.precision:.4f} | Recall: {self.recall:.4f}\\n\")\n",
    "\n",
    "class Result():\n",
    "    def __init__(self, hyperparameters, performance_metrics):\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.performance_metrics = performance_metrics\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (f\"Hyperparameters:\\n\"\n",
    "                f\"Learning Rate: {self.hyperparameters.learning_rate:.4f} | Num Layers: {self.hyperparameters.hidden_layers} | \"\n",
    "                f\"Hidden Sizes: {self.hyperparameters.hidden_sizes} | Batch Size: {self.hyperparameters.batch_size} | \"\n",
    "                f\"Optimizer: {self.hyperparameters.optimizer.__name__} | Activation Function: {self.hyperparameters.activation_function.__name__}\\n\"\n",
    "                f\"\\nPerformance Metrics:\\n\"\n",
    "                f\"Loss: {self.performance_metrics.loss:.4f} | Accuracy: {self.performance_metrics.accuracy:.4f} | \"\n",
    "                f\"Precision: {self.performance_metrics.precision:.4f} | Recall: {self.performance_metrics.recall:.4f}\\n\")\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Tokenizer and Preprocessing Util Functions\n",
    "# ------------------------------------------\n",
    "def create_tokenizer(texts, is_char_level, num_words=None):\n",
    "    \"\"\"\n",
    "    Create and fit a character-level tokenizer.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of texts.\n",
    "        num_words (int or None): Maximum number of tokens to keep.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer: A fitted Tokenizer instance.\n",
    "    \"\"\"\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=is_char_level, lower=True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "def texts_to_bow(tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Convert texts to a bag-of-characters representation.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: A fitted character-level Tokenizer.\n",
    "        texts (list of str): List of texts.\n",
    "\n",
    "    Returns:\n",
    "        Numpy array representing the binary bag-of-characters for each text.\n",
    "    \"\"\"\n",
    "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
    "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "    return matrix\n",
    "\n",
    "def one_hot_encode(labels, num_classes=2):\n",
    "    \"\"\"\n",
    "    Convert numeric labels to one-hot encoded vectors.\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# Hyper-Parameter Random Search\n",
    "def sample_hyperparameters():\n",
    "    \"\"\"\n",
    "    Generate random hyper-parameters for the model.\n",
    "    \"\"\"\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    hidden_layers = [1, 2, 3]\n",
    "    hidden_sizes = [128, 256, 512]\n",
    "    batch_sizes = [32, 64, 128]\n",
    "    optimizers = [tf.keras.optimizers.Adam, tf.keras.optimizers.SGD, tf.keras.optimizers.RMSprop]\n",
    "    activation_functions = [tf.nn.relu, tf.nn.tanh, tf.nn.leaky_relu]\n",
    "\n",
    "    # Randomly sample one value from each category\n",
    "    learning_rate = random.choice(learning_rates)\n",
    "    num_hidden = random.choice(hidden_layers)\n",
    "    picked_size = random.choice(hidden_sizes)\n",
    "    hidden_sizes = [picked_size for i in range(num_hidden)]\n",
    "    batch_size = random.choice(batch_sizes)\n",
    "    optimizer = random.choice(optimizers)\n",
    "    act_func = random.choice(activation_functions)\n",
    "\n",
    "    return Hyperparameters(learning_rate, num_hidden, hidden_sizes, batch_size, optimizer, act_func)\n",
    "    \n",
    "\n",
    "# --------------------\n",
    "# For logging purposes\n",
    "# --------------------\n",
    "def log_hyperparameters(learning_rate, num_layers, hidden_sizes, batch_size, optimizer, act_func, log_filename=\"training_logs.txt\"):\n",
    "    # Get the current timestamp to label the start of the run\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Prepare the log entry for the hyper-parameters\n",
    "    log_entry = (f\"\\nHyper-parameters:\\n\"\n",
    "                 f\"Learning Rate: {learning_rate:.4f} | Num Layers: {num_layers} | Hidden Sizes: {hidden_sizes} | \"\n",
    "                 f\"Batch Size: {batch_size} | Optimizer: {optimizer.__name__} | Activation Function: {act_func.__name__}\\n\")\n",
    "    \n",
    "    # Append to the log file\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(log_entry)\n",
    "\n",
    "def log_training_info(epoch, epoch_loss, val_loss, accuracy, precision, recall, log_filename=\"training_logs.txt\"):\n",
    "    # Get the current timestamp to label the start of the run\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Prepare the log entry for each epoch\n",
    "    log_entry = (f\"\\n[{timestamp}] - Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                 f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\\n\")\n",
    "    \n",
    "    # Append to the log file\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(log_entry)\n",
    "\n",
    "def log_final_evaluation(test_loss, test_accuracy, test_precision, test_recall, log_filename=\"training_logs.txt\"):\n",
    "    # Prepare the final evaluation log entry\n",
    "    final_log_entry = (f\"\\nFinal Evaluation on Test Set:\\n\"\n",
    "                       f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "                       f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\\n\"\n",
    "                       f\"\\n=======================================================================================================================\\n\")\n",
    "    \n",
    "    # Append final evaluation to the log file\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(final_log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# MLP Class Definition\n",
    "# --------------------\n",
    "class MLP(object):\n",
    "    def __init__(self, size_input, num_hidden, hidden_sizes, size_output, batch_size, optimizer, learning_rate, act_func, device=None):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden1: int, size of the 1st hidden layer\n",
    "        size_hidden2: int, size of the 2nd hidden layer\n",
    "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None.\n",
    "        \"\"\"\n",
    "        self.size_input = size_input # Number of features\n",
    "        self.num_hidden = num_hidden # Number of hidden layers\n",
    "        self.hidden_sizes = hidden_sizes # List of hidden layer sizes\n",
    "        self.size_output = size_output # Number of classes (Binary Classification)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer # Optimizer class (e.g. Adam, SGD, RMSprop)\n",
    "        self.act_func = act_func # Activation function to use (e.g. ReLU, Tanh, Leaky ReLU)\n",
    "        self.device = device # Device to run the model on (cpu or gpu)\n",
    "        \n",
    "        self.hidden_layers = []\n",
    "        \n",
    "        w = tf.Variable(tf.random.normal([self.size_input, self.hidden_sizes[0]], stddev=0.1))\n",
    "        b = tf.Variable(tf.zeros([1, self.hidden_sizes[0]]))\n",
    "        self.hidden_layers.append((w, b))\n",
    "        \n",
    "        for i in range(1, self.num_hidden):\n",
    "            w = tf.Variable(tf.random.normal([self.hidden_sizes[i-1], self.hidden_sizes[i]], stddev=0.1))\n",
    "            b = tf.Variable(tf.zeros([1, self.hidden_sizes[i]]))\n",
    "            self.hidden_layers.append((w, b))\n",
    "        \n",
    "        w = tf.Variable(tf.random.normal([self.hidden_sizes[-1], self.size_output], stddev=0.1))\n",
    "        b = tf.Variable(tf.zeros([1, self.size_output]))\n",
    "        self.hidden_layers.append((w, b))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        X: Tensor, inputs.\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "        return self.y\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the loss between predicted and true outputs.\n",
    "        y_pred: Tensor of shape (batch_size, size_output)\n",
    "        y_true: Tensor of shape (batch_size, size_output)\n",
    "        \"\"\"\n",
    "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        loss_x = cce(y_true_tf, y_pred_tf)\n",
    "        return loss_x\n",
    "\n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Backward pass: compute and apply gradients of the loss with respect to the variables.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "        \n",
    "        trainable_vars = [var for w, b in self.hidden_layers for var in (w, b)]    \n",
    "        \n",
    "        grads = tape.gradient(current_loss, trainable_vars)\n",
    "        \n",
    "        optimizer = self.optimizer(learning_rate=self.learning_rate)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "    def compute_output(self, X):\n",
    "        \"\"\"\n",
    "        Custom method to compute the output tensor during the forward pass.\n",
    "        \"\"\"        \n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        z = X_tf\n",
    "        \n",
    "        # Goes through each hidden layer and applies the weights and biases\n",
    "        for w, b in self.hidden_layers:\n",
    "            z = tf.matmul(z, w) + b\n",
    "            \n",
    "            # If the current layer is the last layer, then its the output layer.\n",
    "            if (w, b) == self.hidden_layers[-1]:\n",
    "                return z\n",
    "            \n",
    "            z = self.act_func(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model (Train Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, X_val, y_train, y_val, epochs = 3):\n",
    "    num_batches = int(np.ceil(X_train.shape[0] / model.batch_size))\n",
    "\n",
    "    print(\"\\nStarting training...\\n\")\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data at the start of each epoch.\n",
    "        indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            start = i * model.batch_size\n",
    "            end = min((i+1) * model.batch_size, X_train.shape[0])\n",
    "            X_batch = X_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            # Compute gradients and update weights.\n",
    "            predictions = model.forward(X_batch)\n",
    "            loss_value = model.loss(predictions, y_batch)\n",
    "            model.backward(X_batch, y_batch)\n",
    "\n",
    "            epoch_loss += loss_value.numpy() * (end - start)\n",
    "\n",
    "        epoch_loss /= X_train.shape[0]\n",
    "\n",
    "        # Evaluate on validation set.\n",
    "        val_logits = model.forward(X_val)\n",
    "        val_loss = model.loss(val_logits, y_val).numpy()\n",
    "        val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
    "        true_val = np.argmax(y_val, axis=1)\n",
    "        accuracy = np.mean(val_preds == true_val)\n",
    "        precision = precision_score(true_val, val_preds)\n",
    "        recall = recall_score(true_val, val_preds)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "            f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on Test Set (Test Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_test, y_test):\n",
    "    # -------------------------------\n",
    "    # Final Evaluation on Test Set\n",
    "    # -------------------------------\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_logits = model.forward(X_test)\n",
    "    test_loss = model.loss(test_logits, y_test).numpy()\n",
    "    test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
    "    true_test = np.argmax(y_test, axis=1)\n",
    "    test_accuracy = np.mean(test_preds == true_test)\n",
    "    test_precision = precision_score(true_test, test_preds)\n",
    "    test_recall = recall_score(true_test, test_preds)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "        f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n",
    "    \n",
    "    return test_loss, test_accuracy, test_precision, test_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading IMDB Dataset and Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n",
      "\n",
      "IMBD Dataset loaded:\n",
      "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
      "\n",
      "Tokenizing texts...\n",
      "Char tokenizer vocabulary size: 134\n",
      "Word tokenizer vocabulary size: 80169 \n",
      "\n",
      "Converting texts to bag-of-characters...\n",
      "Converting texts to bag-of-words...\n",
      "Performing one-hot encoding of labels...\n",
      "\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Load and Prepare the IMDB Dataset\n",
    "# -------------------------------\n",
    "print(\"Loading IMDB dataset...\\n\")\n",
    "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
    "                                           split=['train', 'test'],\n",
    "                                           as_supervised=True,\n",
    "                                           with_info=True)\n",
    "\n",
    "# Convert training dataset to lists.\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for text, label in tfds.as_numpy(ds_train):\n",
    "    # Decode byte strings to utf-8 strings.\n",
    "    train_texts.append(text.decode('utf-8'))\n",
    "    train_labels.append(label)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Create a validation set from the training data (20% for validation).\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert test dataset to lists.\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for text, label in tfds.as_numpy(ds_test):\n",
    "    test_texts.append(text.decode('utf-8'))\n",
    "    test_labels.append(label)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(\"IMBD Dataset loaded:\")\n",
    "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Preprocessing: Tokenization and Vectorization\n",
    "# -------------------------------\n",
    "print(\"Tokenizing texts...\")\n",
    "# Build the character-level tokenizer on the training texts.\n",
    "char_tokenizer = create_tokenizer(train_texts, is_char_level=True)\n",
    "print(\"Char tokenizer vocabulary size:\", len(char_tokenizer.word_index) + 1)\n",
    "\n",
    "word_tokenizer = create_tokenizer(train_texts, is_char_level=False, num_words=10000) # Limited to the top 10,000 common words.\n",
    "print(\"Word tokenizer vocabulary size:\", len(word_tokenizer.word_index) + 1, \"\\n\")\n",
    "\n",
    "print(\"Converting texts to bag-of-characters...\")\n",
    "# Convert texts to bag-of-characters representation.\n",
    "X_train_char = texts_to_bow(char_tokenizer, train_texts)\n",
    "X_val_char   = texts_to_bow(char_tokenizer, val_texts)\n",
    "X_test_char  = texts_to_bow(char_tokenizer, test_texts)\n",
    "\n",
    "print(\"Converting texts to bag-of-words...\")\n",
    "# Convert texts to bag-of-words representation.\n",
    "X_train_word = texts_to_bow(word_tokenizer, train_texts)\n",
    "X_val_word   = texts_to_bow(word_tokenizer, val_texts)\n",
    "X_test_word  = texts_to_bow(word_tokenizer, test_texts)\n",
    "\n",
    "print(\"Performing one-hot encoding on labels...\\n\")\n",
    "# Convert labels to one-hot encoding.\n",
    "y_train = one_hot_encode(train_labels)\n",
    "y_val   = one_hot_encode(val_labels)\n",
    "y_test  = one_hot_encode(test_labels)\n",
    "print(\"Preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Tokenization Vs. Word Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the character-level model...\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6782 | Val Loss: 0.6639 | Accuracy: 0.6074 | Precision: 0.5954 | Recall: 0.5932\n",
      "Epoch 02 | Training Loss: 0.6695 | Val Loss: 0.6702 | Accuracy: 0.5876 | Precision: 0.6241 | Recall: 0.3754\n",
      "Epoch 03 | Training Loss: 0.6653 | Val Loss: 0.6780 | Accuracy: 0.5886 | Precision: 0.5529 | Recall: 0.7917\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.6725 | Test Accuracy: 0.5978 | Test Precision: 0.5692 | Test Recall: 0.8040\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Training the word-level model...\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4296 | Val Loss: 0.3437 | Accuracy: 0.8642 | Precision: 0.8115 | Recall: 0.9377\n",
      "Epoch 02 | Training Loss: 0.2606 | Val Loss: 0.2990 | Accuracy: 0.8834 | Precision: 0.8478 | Recall: 0.9257\n",
      "Epoch 03 | Training Loss: 0.2244 | Val Loss: 0.3011 | Accuracy: 0.8900 | Precision: 0.8751 | Recall: 0.9018\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3095 | Test Accuracy: 0.8853 | Test Precision: 0.8814 | Test Recall: 0.8903\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Model Setup\n",
    "# -------------------------------\n",
    "# The input size is determined by the dimension of the bag-of-characters vector.\n",
    "char_size_input = X_train_char.shape[1]\n",
    "word_size_input = X_train_word.shape[1]\n",
    "\n",
    "# Parameters used for both models.\n",
    "num_hidden = 3 # Number of hidden layers\n",
    "hidden_sizes = [128, 64, 32] # Size of each hidden layer\n",
    "batch_size = 128\n",
    "optimizer = tf.keras.optimizers.Adam\n",
    "learning_rate = 0.001\n",
    "act_func = tf.nn.relu # Activation function used\n",
    "size_output  = 2 # Binary classification\n",
    "\n",
    "char_model = MLP(size_input=char_size_input,\n",
    "            num_hidden=num_hidden,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            size_output=size_output,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=optimizer,\n",
    "            learning_rate=learning_rate,\n",
    "            act_func=act_func,\n",
    "            device=None)\n",
    "\n",
    "word_model = MLP(size_input=word_size_input,\n",
    "            num_hidden=num_hidden,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            size_output=size_output,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=optimizer,\n",
    "            learning_rate=learning_rate,\n",
    "            act_func=act_func,\n",
    "            device=None)\n",
    "\n",
    "# -------------------------------\n",
    "# Training and Evaluation (Testing)\n",
    "# -------------------------------\n",
    "\n",
    "print(\"\\nTraining the character-level model...\")\n",
    "train_model(char_model, X_train_char, X_val_char, y_train, y_val)\n",
    "test_model(char_model, X_test_char, y_test)\n",
    "\n",
    "print(\"\\n-----------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"\\nTraining the word-level model...\")\n",
    "train_model(word_model, X_train_word, X_val_word, y_train, y_val)\n",
    "test_model(word_model, X_test_word, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper_parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.0005 | Num Layers: 1 | Hidden Sizes: [128] | Batch Size: 32 | Optimizer: RMSprop | Activation Function: relu\n",
      "\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.3790 | Val Loss: 0.3205 | Accuracy: 0.8734 | Precision: 0.8651 | Recall: 0.8754\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3148 | Test Accuracy: 0.8759 | Test Precision: 0.8780 | Test Recall: 0.8732\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "Hyperparameters:\n",
      "Learning Rate: 0.0005 | Num Layers: 1 | Hidden Sizes: [128] | Batch Size: 32 | Optimizer: RMSprop | Activation Function: relu\n",
      "Performance Metrics:\n",
      "Loss: 0.3148 | Accuracy: 0.8759 | Precision: 0.8780 | Recall: 0.8732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "# -------------------------------\n",
    "# Model Setup\n",
    "# -------------------------------\n",
    "# The input size is determined by the dimension of the bag-of-characters vector.\n",
    "size_input = X_train_word.shape[1]\n",
    "size_output  = 2 # Binary Classification\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = Hyperparameters()\n",
    "params.sample_random()\n",
    "print(params)\n",
    "\n",
    "\n",
    "model = MLP(size_input = size_input,\n",
    "            num_hidden = params.hidden_layers,\n",
    "            hidden_sizes = params.hidden_sizes,\n",
    "            size_output = size_output,\n",
    "            batch_size = params.batch_size,\n",
    "            optimizer = params.optimizer,\n",
    "            learning_rate = params.learning_rate,\n",
    "            act_func = params.activation_function,\n",
    "            device=None)\n",
    "\n",
    "\n",
    "train_model(model, X_train_word, X_val_word, y_train, y_val, epochs=1)\n",
    "pm = PerformanceMetrics(*test_model(model, X_test_word, y_test))\n",
    "results.append(Result(params, pm))\n",
    "\n",
    "print(\"\\n-----------------------------------------------------------------------------------------------------------------------\")\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = MLP(size_input=size_input,\n",
    "#             num_hidden=num_hidden,\n",
    "#             hidden_sizes=hidden_sizes,\n",
    "#             size_output=size_output,\n",
    "#             batch_size=batch_size,\n",
    "#             optimizer=optimizer,\n",
    "#             learning_rate=learning_rate,\n",
    "#             act_func=act_func,\n",
    "#             device=None)\n",
    "\n",
    "# -------------------------------\n",
    "# Training Parameters and Loop\n",
    "# -------------------------------\n",
    "# batch_size = 128\n",
    "# for i in range(3):\n",
    "#     epochs = 3\n",
    "#     num_batches = int(np.ceil(X_train_word.shape[0] / model.batch_size))\n",
    "    \n",
    "#     log_hyperparameters(learning_rate, num_hidden, hidden_sizes, batch_size, optimizer, act_func)\n",
    "\n",
    "#     print(\"\\nStarting training...\\n\")\n",
    "#     for epoch in range(epochs):\n",
    "#         # Shuffle training data at the start of each epoch.\n",
    "#         indices = np.arange(X_train_word.shape[0])\n",
    "#         np.random.shuffle(indices)\n",
    "#         X_train_word = X_train_word[indices]\n",
    "#         y_train = y_train[indices]\n",
    "\n",
    "#         epoch_loss = 0\n",
    "#         for i in range(num_batches):\n",
    "#             start = i * model.batch_size\n",
    "#             end = min((i+1) * model.batch_size, X_train_word.shape[0])\n",
    "#             X_batch = X_train_word[start:end]\n",
    "#             y_batch = y_train[start:end]\n",
    "\n",
    "#             # Compute gradients and update weights.\n",
    "#             predictions = model.forward(X_batch)\n",
    "#             loss_value = model.loss(predictions, y_batch)\n",
    "#             model.backward(X_batch, y_batch)\n",
    "\n",
    "#             epoch_loss += loss_value.numpy() * (end - start)\n",
    "\n",
    "#         epoch_loss /= X_train_word.shape[0]\n",
    "\n",
    "#         # Evaluate on validation set.\n",
    "#         val_logits = model.forward(X_val_char)\n",
    "#         val_loss = model.loss(val_logits, y_val).numpy()\n",
    "#         val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
    "#         true_val = np.argmax(y_val, axis=1)\n",
    "#         accuracy = np.mean(val_preds == true_val)\n",
    "#         precision = precision_score(true_val, val_preds)\n",
    "#         recall = recall_score(true_val, val_preds)\n",
    "\n",
    "#         print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "#             f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "        \n",
    "#         log_training_info(epoch, epoch_loss, val_loss, accuracy, precision, recall)\n",
    "\n",
    "#     # -------------------------------\n",
    "#     # Final Evaluation on Test Set\n",
    "#     # -------------------------------\n",
    "#     print(\"\\nEvaluating on test set...\")\n",
    "#     test_logits = model.forward(X_test_char)\n",
    "#     test_loss = model.loss(test_logits, y_test).numpy()\n",
    "#     test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
    "#     true_test = np.argmax(y_test, axis=1)\n",
    "#     test_accuracy = np.mean(test_preds == true_test)\n",
    "#     test_precision = precision_score(true_test, test_preds)\n",
    "#     test_recall = recall_score(true_test, test_preds)\n",
    "\n",
    "#     print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "#         f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "#     log_final_evaluation(test_loss, test_accuracy, test_precision, test_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

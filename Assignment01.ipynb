{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01: [creative title name]\n",
    "\n",
    "### Maxim Ryabinov (U02204083)\n",
    "### CAP4641: Natural Language Processing \n",
    "### Instructor: Dr. Ankur Mali \n",
    "### University of South Florida (Spring 2025)\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Initial Setup\n",
    "- Sets random seeds to ensure reproducibility.\n",
    "- Creates \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Suppress TensorFlow warnings.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(123)\n",
    "\n",
    "# -------------------------------\n",
    "# Tokenizer and Preprocessing Util Functions\n",
    "# -------------------------------\n",
    "def create_tokenizer(texts, is_char_level, num_words=None):\n",
    "    \"\"\"\n",
    "    Create and fit a character-level tokenizer.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of texts.\n",
    "        num_words (int or None): Maximum number of tokens to keep.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer: A fitted Tokenizer instance.\n",
    "    \"\"\"\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=is_char_level, lower=True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "def texts_to_bow(tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Convert texts to a bag-of-characters representation.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: A fitted character-level Tokenizer.\n",
    "        texts (list of str): List of texts.\n",
    "\n",
    "    Returns:\n",
    "        Numpy array representing the binary bag-of-characters for each text.\n",
    "    \"\"\"\n",
    "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
    "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "    return matrix\n",
    "\n",
    "def one_hot_encode(labels, num_classes=2):\n",
    "    \"\"\"\n",
    "    Convert numeric labels to one-hot encoded vectors.\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# Hyper-Parameter Random Search\n",
    "def sample_hyperparameters():\n",
    "    \"\"\"\n",
    "    Generate random hyper-parameters for the model.\n",
    "    \"\"\"\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    hidden_layers = [1, 2, 3]\n",
    "    hidden_sizes = [128, 256, 512]\n",
    "    batch_sizes = [32, 64, 128]\n",
    "    optimizers = [tf.keras.optimizers.Adam, tf.keras.optimizers.SGD, tf.keras.optimizers.RMSprop]\n",
    "    activation_functions = [tf.nn.relu, tf.nn.tanh, tf.nn.leaky_relu]\n",
    "\n",
    "    # Randomly sample one value from each category\n",
    "    learning_rate = random.choice(learning_rates)\n",
    "    num_hidden = random.choice(hidden_layers)\n",
    "    picked_size = random.choice(hidden_sizes)\n",
    "    hidden_sizes = [picked_size for i in range(num_hidden)]\n",
    "    batch_size = random.choice(batch_sizes)\n",
    "    optimizer = random.choice(optimizers)\n",
    "    act_func = random.choice(activation_functions)\n",
    "\n",
    "    return learning_rate, num_hidden, hidden_sizes, batch_size, optimizer, act_func\n",
    "    \n",
    "\n",
    "# -------------------------------\n",
    "# For logging purposes\n",
    "# -------------------------------\n",
    "def log_hyperparameters(learning_rate, num_layers, hidden_sizes, batch_size, optimizer, act_func, log_filename=\"training_logs.txt\"):\n",
    "    # Get the current timestamp to label the start of the run\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Prepare the log entry for the hyper-parameters\n",
    "    log_entry = (f\"\\nHyper-parameters:\\n\"\n",
    "                 f\"Learning Rate: {learning_rate:.4f} | Num Layers: {num_layers} | Hidden Sizes: {hidden_sizes} | \"\n",
    "                 f\"Batch Size: {batch_size} | Optimizer: {optimizer.__name__} | Activation Function: {act_func.__name__}\\n\")\n",
    "    \n",
    "    # Append to the log file\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(log_entry)\n",
    "\n",
    "def log_training_info(epoch, epoch_loss, val_loss, accuracy, precision, recall, log_filename=\"training_logs.txt\"):\n",
    "    # Get the current timestamp to label the start of the run\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Prepare the log entry for each epoch\n",
    "    log_entry = (f\"\\n[{timestamp}] - Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                 f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\\n\")\n",
    "    \n",
    "    # Append to the log file\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(log_entry)\n",
    "\n",
    "def log_final_evaluation(test_loss, test_accuracy, test_precision, test_recall, log_filename=\"training_logs.txt\"):\n",
    "    # Prepare the final evaluation log entry\n",
    "    final_log_entry = (f\"\\nFinal Evaluation on Test Set:\\n\"\n",
    "                       f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "                       f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\\n\"\n",
    "                       f\"\\n=======================================================================================================================\\n\")\n",
    "    \n",
    "    # Append final evaluation to the log file\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(final_log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# MLP Class Definition\n",
    "# -------------------------------\n",
    "class MLP(object):\n",
    "    def __init__(self, size_input, num_hidden, hidden_sizes, size_output, batch_size, optimizer, learning_rate, act_func, device=None):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden1: int, size of the 1st hidden layer\n",
    "        size_hidden2: int, size of the 2nd hidden layer\n",
    "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None.\n",
    "        \"\"\"\n",
    "        self.size_input = size_input # Number of features\n",
    "        self.num_hidden = num_hidden # Number of hidden layers\n",
    "        self.hidden_sizes = hidden_sizes # List of hidden layer sizes\n",
    "        self.size_output = size_output # Number of classes (Binary Classification)\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer # Optimizer class (e.g. Adam, SGD, RMSprop)\n",
    "        self.act_func = act_func # Activation function to use (e.g. ReLU, Tanh, Leaky ReLU)\n",
    "        self.device = device # Device to run the model on (cpu or gpu)\n",
    "        \n",
    "        self.hidden_layers = []\n",
    "        \n",
    "        w = tf.Variable(tf.random.normal([self.size_input, self.hidden_sizes[0]], stddev=0.1))\n",
    "        b = tf.Variable(tf.zeros([1, self.hidden_sizes[0]]))\n",
    "        self.hidden_layers.append((w, b))\n",
    "        \n",
    "        for i in range(1, self.num_hidden):\n",
    "            w = tf.Variable(tf.random.normal([self.hidden_sizes[i-1], self.hidden_sizes[i]], stddev=0.1))\n",
    "            b = tf.Variable(tf.zeros([1, self.hidden_sizes[i]]))\n",
    "            self.hidden_layers.append((w, b))\n",
    "        \n",
    "        w = tf.Variable(tf.random.normal([self.hidden_sizes[-1], self.size_output], stddev=0.1))\n",
    "        b = tf.Variable(tf.zeros([1, self.size_output]))\n",
    "        self.hidden_layers.append((w, b))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        X: Tensor, inputs.\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "        return self.y\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the loss between predicted and true outputs.\n",
    "        y_pred: Tensor of shape (batch_size, size_output)\n",
    "        y_true: Tensor of shape (batch_size, size_output)\n",
    "        \"\"\"\n",
    "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        loss_x = cce(y_true_tf, y_pred_tf)\n",
    "        return loss_x\n",
    "\n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Backward pass: compute and apply gradients of the loss with respect to the variables.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "        \n",
    "        trainable_vars = [var for w, b in self.hidden_layers for var in (w, b)]    \n",
    "        \n",
    "        grads = tape.gradient(current_loss, trainable_vars)\n",
    "        \n",
    "        optimizer = self.optimizer(learning_rate=self.learning_rate)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "    def compute_output(self, X):\n",
    "        \"\"\"\n",
    "        Custom method to compute the output tensor during the forward pass.\n",
    "        \"\"\"        \n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        z = X_tf\n",
    "        \n",
    "        # Goes through each hidden layer and applies the weights and biases\n",
    "        for w, b in self.hidden_layers:\n",
    "            z = tf.matmul(z, w) + b\n",
    "            \n",
    "            # If the current layer is the last layer, then its the output layer.\n",
    "            if (w, b) == self.hidden_layers[-1]:\n",
    "                return z\n",
    "            \n",
    "            z = self.act_func(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model (Train Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, X_val, y_train, y_val, epochs = 3):\n",
    "    num_batches = int(np.ceil(X_train.shape[0] / model.batch_size))\n",
    "\n",
    "    print(\"\\nStarting training...\\n\")\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data at the start of each epoch.\n",
    "        indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            start = i * model.batch_size\n",
    "            end = min((i+1) * model.batch_size, X_train.shape[0])\n",
    "            X_batch = X_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            # Compute gradients and update weights.\n",
    "            predictions = model.forward(X_batch)\n",
    "            loss_value = model.loss(predictions, y_batch)\n",
    "            model.backward(X_batch, y_batch)\n",
    "\n",
    "            epoch_loss += loss_value.numpy() * (end - start)\n",
    "\n",
    "        epoch_loss /= X_train.shape[0]\n",
    "\n",
    "        # Evaluate on validation set.\n",
    "        val_logits = model.forward(X_val)\n",
    "        val_loss = model.loss(val_logits, y_val).numpy()\n",
    "        val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
    "        true_val = np.argmax(y_val, axis=1)\n",
    "        accuracy = np.mean(val_preds == true_val)\n",
    "        precision = precision_score(true_val, val_preds)\n",
    "        recall = recall_score(true_val, val_preds)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "            f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on Test Set (Test Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_test, y_test):\n",
    "    # -------------------------------\n",
    "    # Final Evaluation on Test Set\n",
    "    # -------------------------------\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_logits = model.forward(X_test)\n",
    "    test_loss = model.loss(test_logits, y_test).numpy()\n",
    "    test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
    "    true_test = np.argmax(y_test, axis=1)\n",
    "    test_accuracy = np.mean(test_preds == true_test)\n",
    "    test_precision = precision_score(true_test, test_preds)\n",
    "    test_recall = recall_score(true_test, test_preds)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "        f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading IMDB Dataset and Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n",
      "\n",
      "IMBD Dataset loaded:\n",
      "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
      "\n",
      "Tokenizing texts...\n",
      "Char tokenizer vocabulary size: 134\n",
      "Word tokenizer vocabulary size: 80169 \n",
      "\n",
      "Converting texts to bag-of-characters...\n",
      "Converting texts to bag-of-words...\n",
      "Performing one-hot encoding of labels...\n",
      "\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Load and Prepare the IMDB Dataset\n",
    "# -------------------------------\n",
    "print(\"Loading IMDB dataset...\\n\")\n",
    "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
    "                                           split=['train', 'test'],\n",
    "                                           as_supervised=True,\n",
    "                                           with_info=True)\n",
    "\n",
    "# Convert training dataset to lists.\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for text, label in tfds.as_numpy(ds_train):\n",
    "    # Decode byte strings to utf-8 strings.\n",
    "    train_texts.append(text.decode('utf-8'))\n",
    "    train_labels.append(label)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Create a validation set from the training data (20% for validation).\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert test dataset to lists.\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for text, label in tfds.as_numpy(ds_test):\n",
    "    test_texts.append(text.decode('utf-8'))\n",
    "    test_labels.append(label)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(\"IMBD Dataset loaded:\")\n",
    "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Preprocessing: Tokenization and Vectorization\n",
    "# -------------------------------\n",
    "print(\"Tokenizing texts...\")\n",
    "# Build the character-level tokenizer on the training texts.\n",
    "char_tokenizer = create_tokenizer(train_texts, is_char_level=True)\n",
    "print(\"Char tokenizer vocabulary size:\", len(char_tokenizer.word_index) + 1)\n",
    "\n",
    "word_tokenizer = create_tokenizer(train_texts, is_char_level=False, num_words=10000) # Limited to the top 10,000 common words.\n",
    "print(\"Word tokenizer vocabulary size:\", len(word_tokenizer.word_index) + 1, \"\\n\")\n",
    "\n",
    "print(\"Converting texts to bag-of-characters...\")\n",
    "# Convert texts to bag-of-characters representation.\n",
    "X_train_char = texts_to_bow(char_tokenizer, train_texts)\n",
    "X_val_char   = texts_to_bow(char_tokenizer, val_texts)\n",
    "X_test_char  = texts_to_bow(char_tokenizer, test_texts)\n",
    "\n",
    "print(\"Converting texts to bag-of-words...\")\n",
    "# Convert texts to bag-of-words representation.\n",
    "X_train_word = texts_to_bow(word_tokenizer, train_texts)\n",
    "X_val_word   = texts_to_bow(word_tokenizer, val_texts)\n",
    "X_test_word  = texts_to_bow(word_tokenizer, test_texts)\n",
    "\n",
    "print(\"Performing one-hot encoding on labels...\\n\")\n",
    "# Convert labels to one-hot encoding.\n",
    "y_train = one_hot_encode(train_labels)\n",
    "y_val   = one_hot_encode(val_labels)\n",
    "y_test  = one_hot_encode(test_labels)\n",
    "print(\"Preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Tokenization Vs. Word Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the character-level model...\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6782 | Val Loss: 0.6639 | Accuracy: 0.6074 | Precision: 0.5954 | Recall: 0.5932\n",
      "Epoch 02 | Training Loss: 0.6695 | Val Loss: 0.6702 | Accuracy: 0.5876 | Precision: 0.6241 | Recall: 0.3754\n",
      "Epoch 03 | Training Loss: 0.6653 | Val Loss: 0.6780 | Accuracy: 0.5886 | Precision: 0.5529 | Recall: 0.7917\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.6725 | Test Accuracy: 0.5978 | Test Precision: 0.5692 | Test Recall: 0.8040\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Training the word-level model...\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4296 | Val Loss: 0.3437 | Accuracy: 0.8642 | Precision: 0.8115 | Recall: 0.9377\n",
      "Epoch 02 | Training Loss: 0.2606 | Val Loss: 0.2990 | Accuracy: 0.8834 | Precision: 0.8478 | Recall: 0.9257\n",
      "Epoch 03 | Training Loss: 0.2244 | Val Loss: 0.3011 | Accuracy: 0.8900 | Precision: 0.8751 | Recall: 0.9018\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.3095 | Test Accuracy: 0.8853 | Test Precision: 0.8814 | Test Recall: 0.8903\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Model Setup\n",
    "# -------------------------------\n",
    "# The input size is determined by the dimension of the bag-of-characters vector.\n",
    "char_size_input = X_train_char.shape[1]\n",
    "word_size_input = X_train_word.shape[1]\n",
    "\n",
    "# Parameters used for both models.\n",
    "num_hidden = 3 # Number of hidden layers\n",
    "hidden_sizes = [128, 64, 32] # Size of each hidden layer\n",
    "batch_size = 128\n",
    "optimizer = tf.keras.optimizers.Adam\n",
    "learning_rate = 0.001\n",
    "act_func = tf.nn.relu # Activation function used\n",
    "size_output  = 2 # Binary classification\n",
    "\n",
    "char_model = MLP(size_input=char_size_input,\n",
    "            num_hidden=num_hidden,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            size_output=size_output,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=optimizer,\n",
    "            learning_rate=learning_rate,\n",
    "            act_func=act_func,\n",
    "            device=None)\n",
    "\n",
    "word_model = MLP(size_input=word_size_input,\n",
    "            num_hidden=num_hidden,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            size_output=size_output,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=optimizer,\n",
    "            learning_rate=learning_rate,\n",
    "            act_func=act_func,\n",
    "            device=None)\n",
    "\n",
    "# -------------------------------\n",
    "# Training and Evaluation (Testing)\n",
    "# -------------------------------\n",
    "\n",
    "print(\"\\nTraining the character-level model...\")\n",
    "train_model(char_model, X_train_char, X_val_char, y_train, y_val)\n",
    "test_model(char_model, X_test_char, y_test)\n",
    "\n",
    "print(\"\\n-----------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"\\nTraining the word-level model...\")\n",
    "train_model(word_model, X_train_word, X_val_word, y_train, y_val)\n",
    "test_model(word_model, X_test_word, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 11:03:36.498743: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3206760000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Training Loss: 0.3591 | Val Loss: 0.3041 | Accuracy: 0.8682 | Precision: 0.8265 | Recall: 0.9216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 11:04:25.294416: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3206760000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Training Loss: 0.2015 | Val Loss: 0.3112 | Accuracy: 0.8718 | Precision: 0.9062 | Recall: 0.8205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 11:05:08.915377: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3206760000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | Training Loss: 0.1378 | Val Loss: 0.3190 | Accuracy: 0.8768 | Precision: 0.8482 | Recall: 0.9084\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Model Setup\n",
    "# -------------------------------\n",
    "# The input size is determined by the dimension of the bag-of-characters vector.\n",
    "size_input = X_train_word.shape[1]\n",
    "\n",
    "\n",
    "learning_rate, num_hidden, hidden_sizes, batch_size, optimizer, act_func = sample_hyperparameters()\n",
    "size_output  = 2 # Binary Classification\n",
    "\n",
    "model = MLP(size_input=size_input,\n",
    "            num_hidden=num_hidden,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            size_output=size_output,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=optimizer,\n",
    "            learning_rate=learning_rate,\n",
    "            act_func=act_func,\n",
    "            device=None)\n",
    "\n",
    "# -------------------------------\n",
    "# Training Parameters and Loop\n",
    "# -------------------------------\n",
    "# batch_size = 128\n",
    "for i in range(3):\n",
    "    epochs = 3\n",
    "    num_batches = int(np.ceil(X_train_word.shape[0] / model.batch_size))\n",
    "    \n",
    "    log_hyperparameters(learning_rate, num_hidden, hidden_sizes, batch_size, optimizer, act_func)\n",
    "\n",
    "    print(\"\\nStarting training...\\n\")\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data at the start of each epoch.\n",
    "        indices = np.arange(X_train_word.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train_word = X_train_word[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            start = i * model.batch_size\n",
    "            end = min((i+1) * model.batch_size, X_train_word.shape[0])\n",
    "            X_batch = X_train_word[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            # Compute gradients and update weights.\n",
    "            predictions = model.forward(X_batch)\n",
    "            loss_value = model.loss(predictions, y_batch)\n",
    "            model.backward(X_batch, y_batch)\n",
    "\n",
    "            epoch_loss += loss_value.numpy() * (end - start)\n",
    "\n",
    "        epoch_loss /= X_train_word.shape[0]\n",
    "\n",
    "        # Evaluate on validation set.\n",
    "        val_logits = model.forward(X_val_char)\n",
    "        val_loss = model.loss(val_logits, y_val).numpy()\n",
    "        val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
    "        true_val = np.argmax(y_val, axis=1)\n",
    "        accuracy = np.mean(val_preds == true_val)\n",
    "        precision = precision_score(true_val, val_preds)\n",
    "        recall = recall_score(true_val, val_preds)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "            f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "        \n",
    "        log_training_info(epoch, epoch_loss, val_loss, accuracy, precision, recall)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Final Evaluation on Test Set\n",
    "    # -------------------------------\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_logits = model.forward(X_test_char)\n",
    "    test_loss = model.loss(test_logits, y_test).numpy()\n",
    "    test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
    "    true_test = np.argmax(y_test, axis=1)\n",
    "    test_accuracy = np.mean(test_preds == true_test)\n",
    "    test_precision = precision_score(true_test, test_preds)\n",
    "    test_recall = recall_score(true_test, test_preds)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "        f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "    log_final_evaluation(test_loss, test_accuracy, test_precision, test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739845240.442899   10818 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739845240.447638   10818 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n",
      "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
      "Tokenizer vocabulary size: 80170\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.3999 | Val Loss: 0.2916 | Accuracy: 0.8812 | Precision: 0.9038 | Recall: 0.8449\n",
      "Epoch 02 | Training Loss: 0.1371 | Val Loss: 0.3231 | Accuracy: 0.8788 | Precision: 0.8399 | Recall: 0.9266\n",
      "Epoch 03 | Training Loss: 0.0385 | Val Loss: 0.4014 | Accuracy: 0.8812 | Precision: 0.8848 | Recall: 0.8680\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.4700 | Test Accuracy: 0.8649 | Test Precision: 0.8863 | Test Recall: 0.8373\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.4027 | Val Loss: 0.2973 | Accuracy: 0.8808 | Precision: 0.8621 | Recall: 0.8977\n",
      "Epoch 02 | Training Loss: 0.1363 | Val Loss: 0.3119 | Accuracy: 0.8800 | Precision: 0.8806 | Recall: 0.8705\n",
      "Epoch 03 | Training Loss: 0.0371 | Val Loss: 0.4137 | Accuracy: 0.8780 | Precision: 0.8850 | Recall: 0.8601\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.4655 | Test Accuracy: 0.8615 | Test Precision: 0.8875 | Test Recall: 0.8280\n",
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10818/560815448.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;31m#     loss_value = model.loss(predictions, y_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# grads = tape.gradient(loss_value, model.variables)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10818/560815448.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \"\"\"\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/USF-Spring-2025/NLP/Assignment 01/nlp-venv/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1062\u001b[0m               output_gradients))\n\u001b[1;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/USF-Spring-2025/NLP/Assignment 01/nlp-venv/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ValueError(\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/USF-Spring-2025/NLP/Assignment 01/nlp-venv/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/USF-Spring-2025/NLP/Assignment 01/nlp-venv/lib/python3.10/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_ReluGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/USF-Spring-2025/NLP/Assignment 01/nlp-venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(gradients, features, name)\u001b[0m\n\u001b[1;32m  11785\u001b[0m         _ctx, \"ReluGrad\", name, gradients, features)\n\u001b[1;32m  11786\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11787\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11788\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11789\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11790\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11791\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11792\u001b[0m       return relu_grad_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Suppress TensorFlow warnings.\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "# -------------------------------\n",
    "# Original MLP Class Definition\n",
    "# -------------------------------\n",
    "class MLP(object):\n",
    "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden1: int, size of the 1st hidden layer\n",
    "        size_hidden2: int, size of the 2nd hidden layer\n",
    "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None.\n",
    "        \"\"\"\n",
    "        self.size_input = size_input\n",
    "        self.size_hidden1 = size_hidden1\n",
    "        self.size_hidden2 = size_hidden2\n",
    "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
    "        self.size_output = size_output\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize weights and biases for first hidden layer\n",
    "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
    "\n",
    "        # Initialize weights and biases for second hidden layer\n",
    "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "\n",
    "        # Initialize weights and biases for output layer\n",
    "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
    "\n",
    "        # List of variables to update during backpropagation\n",
    "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        X: Tensor, inputs.\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "        return self.y\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the loss between predicted and true outputs.\n",
    "        y_pred: Tensor of shape (batch_size, size_output)\n",
    "        y_true: Tensor of shape (batch_size, size_output)\n",
    "        \"\"\"\n",
    "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        loss_x = cce(y_true_tf, y_pred_tf)\n",
    "        return loss_x\n",
    "\n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients of the loss with respect to the variables.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "        grads = tape.gradient(current_loss, self.variables)\n",
    "        return grads\n",
    "\n",
    "    def compute_output(self, X):\n",
    "        \"\"\"\n",
    "        Custom method to compute the output tensor during the forward pass.\n",
    "        \"\"\"\n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        # First hidden layer\n",
    "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        z1 = tf.nn.relu(h1)\n",
    "        # Second hidden layer\n",
    "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
    "        z2 = tf.nn.relu(h2)\n",
    "        # Output layer (logits)\n",
    "        output = tf.matmul(z2, self.W3) + self.b3\n",
    "        return output\n",
    "\n",
    "def word_level_tokenizer(texts, num_words=None):\n",
    "    \"\"\"\n",
    "    Create and fit a word-level tokenizer.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of texts.\n",
    "        num_words (int or None): Maximum number of tokens to keep.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer: A fitted Tokenizer instance.\n",
    "    \"\"\"\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, lower=True, oov_token=\"<OOV>\", char_level=False)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def texts_to_bow(tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Convert texts to a bag-of-words representation.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: A fitted word-level Tokenizer.\n",
    "        texts (list of str): List of texts.\n",
    "\n",
    "    Returns:\n",
    "        Numpy array representing the binary bag-of-words for each text.\n",
    "    \"\"\"\n",
    "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "    return matrix\n",
    "\n",
    "def one_hot_encode(labels, num_classes=2):\n",
    "    \"\"\"\n",
    "    Convert numeric labels to one-hot encoded vectors.\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# -------------------------------\n",
    "# Load and Prepare the IMDB Dataset\n",
    "# -------------------------------\n",
    "print(\"Loading IMDB dataset...\")\n",
    "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
    "                                           split=['train', 'test'],\n",
    "                                           as_supervised=True,\n",
    "                                           with_info=True)\n",
    "\n",
    "# Convert training dataset to lists.\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for text, label in tfds.as_numpy(ds_train):\n",
    "    # Decode byte strings to utf-8 strings.\n",
    "    train_texts.append(text.decode('utf-8'))\n",
    "    train_labels.append(label)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Create a validation set from the training data (20% for validation).\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert test dataset to lists.\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for text, label in tfds.as_numpy(ds_test):\n",
    "    test_texts.append(text.decode('utf-8'))\n",
    "    test_labels.append(label)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Preprocessing: Tokenization and Vectorization\n",
    "# -------------------------------\n",
    "# Build the word-level tokenizer on the training texts.\n",
    "tokenizer = word_level_tokenizer(train_texts, num_words=20000)\n",
    "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Convert texts to bag-of-words representation.\n",
    "X_train = texts_to_bow(tokenizer, train_texts)\n",
    "X_val   = texts_to_bow(tokenizer, val_texts)\n",
    "X_test  = texts_to_bow(tokenizer, test_texts)\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoding.\n",
    "y_train = one_hot_encode(train_labels)\n",
    "y_val   = one_hot_encode(val_labels)\n",
    "y_test  = one_hot_encode(test_labels)\n",
    "\n",
    "# -------------------------------\n",
    "# Model Setup\n",
    "# -------------------------------\n",
    "# The input size is determined by the dimension of the bag-of-characters vector.\n",
    "size_input = X_train.shape[1]\n",
    "# Set hidden layer sizes as desired.\n",
    "size_hidden1 = 128\n",
    "size_hidden2 = 64\n",
    "size_hidden3 = 32  # Placeholder (not used in the forward pass) 0.001 relu Adam\n",
    "size_output  = 2\n",
    "\n",
    "for i in range(3):\n",
    "    # Instantiate the MLP model.\n",
    "    model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
    "\n",
    "    # Define the optimizer.\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Training Parameters and Loop\n",
    "    # -------------------------------\n",
    "    batch_size = 128\n",
    "    epochs = 3\n",
    "    num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "\n",
    "\n",
    "    print(\"\\nStarting training...\\n\")\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data at the start of each epoch.\n",
    "        indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = min((i+1) * batch_size, X_train.shape[0])\n",
    "            X_batch = X_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            # Compute gradients and update weights.\n",
    "            # with tf.GradientTape() as tape:\n",
    "            #     predictions = model.forward(X_batch)\n",
    "            #     loss_value = model.loss(predictions, y_batch)\n",
    "            # grads = tape.gradient(loss_value, model.variables)\n",
    "            predictions = model.forward(X_batch)\n",
    "            loss_value = model.loss(predictions, y_batch)\n",
    "            grads = model.backward(X_batch, y_batch)\n",
    "            optimizer.apply_gradients(zip(grads, model.variables))\n",
    "            epoch_loss += loss_value.numpy() * (end - start)\n",
    "\n",
    "        epoch_loss /= X_train.shape[0]\n",
    "\n",
    "        # Evaluate on validation set.\n",
    "        val_logits = model.forward(X_val)\n",
    "        val_loss = model.loss(val_logits, y_val).numpy()\n",
    "        val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
    "        true_val = np.argmax(y_val, axis=1)\n",
    "        accuracy = np.mean(val_preds == true_val)\n",
    "        precision = precision_score(true_val, val_preds)\n",
    "        recall = recall_score(true_val, val_preds)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "            f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Final Evaluation on Test Set\n",
    "    # -------------------------------\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_logits = model.forward(X_test)\n",
    "    test_loss = model.loss(test_logits, y_test).numpy()\n",
    "    test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
    "    true_test = np.argmax(y_test, axis=1)\n",
    "    test_accuracy = np.mean(test_preds == true_test)\n",
    "    test_precision = precision_score(true_test, test_preds)\n",
    "    test_recall = recall_score(true_test, test_preds)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "        f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

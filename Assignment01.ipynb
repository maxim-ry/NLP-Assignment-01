{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01: [creative title name]\n",
    "\n",
    "### Maxim Ryabinov (U02204083)\n",
    "### CAP4641: Natural Language Processing \n",
    "### Instructor: Dr. Ankur Mali \n",
    "### University of South Florida (Spring 2025)\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Initial Setup\n",
    "- Sets random seeds to ensure reproducibility.\n",
    "- Creates \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n",
      "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(123)\n",
    "\n",
    "# -------------------------------\n",
    "# Character-Level Tokenizer and Preprocessing Functions\n",
    "# -------------------------------\n",
    "def create_tokenizer(texts, is_char_level, num_words=None):\n",
    "    \"\"\"\n",
    "    Create and fit a character-level tokenizer.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of texts.\n",
    "        num_words (int or None): Maximum number of tokens to keep.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer: A fitted Tokenizer instance.\n",
    "    \"\"\"\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=is_char_level, lower=True)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "def texts_to_bow(tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Convert texts to a bag-of-characters representation.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: A fitted character-level Tokenizer.\n",
    "        texts (list of str): List of texts.\n",
    "\n",
    "    Returns:\n",
    "        Numpy array representing the binary bag-of-characters for each text.\n",
    "    \"\"\"\n",
    "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
    "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
    "    return matrix\n",
    "\n",
    "def one_hot_encode(labels, num_classes=2):\n",
    "    \"\"\"\n",
    "    Convert numeric labels to one-hot encoded vectors.\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# Hyper-Parameter Random Search\n",
    "def sample_hyperparameters():\n",
    "    \"\"\"\n",
    "    Generate random hyper-parameters for the model.\n",
    "    \"\"\"\n",
    "    learning_rates = [0.001, 0.0005, 0.0001]\n",
    "    hidden_layers = [1, 2, 3]\n",
    "    hidden_sizes = [128, 256, 512]\n",
    "    batch_sizes = [32, 64, 128]\n",
    "    optimizers = [tf.keras.optimizers.Adam, tf.keras.optimizers.SGD, tf.keras.optimizers.RMSprop]\n",
    "    activation_functions = [tf.nn.relu, tf.nn.tanh, tf.nn.leaky_relu]\n",
    "\n",
    "    # Randomly sample one value from each category\n",
    "    learning_rate = random.choice(learning_rates)\n",
    "    num_layers = random.choice(hidden_layers)\n",
    "    picked_size = random.choice(hidden_sizes)\n",
    "    hidden_sizes = [picked_size for i in range(num_layers)]\n",
    "    batch_size = random.choice(batch_sizes)\n",
    "    optimizer = random.choice(optimizers)\n",
    "    act_func = random.choice(activation_functions)\n",
    "\n",
    "    return learning_rate, num_layers, hidden_sizes, batch_size, optimizer, act_func\n",
    "    \n",
    "\n",
    "# -------------------------------\n",
    "# For logging purposes\n",
    "# -------------------------------\n",
    "def log_training_info(epoch, epoch_loss, val_loss, accuracy, precision, recall, log_filename=\"training_logs.txt\"):\n",
    "    # Get the current timestamp to label the start of the run\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Prepare the log entry for each epoch\n",
    "    log_entry = (f\"\\n[{timestamp}] - Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                 f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\\n\")\n",
    "    \n",
    "    # Append to the log file\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(log_entry)\n",
    "\n",
    "def log_final_evaluation(test_loss, test_accuracy, test_precision, test_recall, log_filename=\"training_logs.txt\"):\n",
    "    # Get the current timestamp to label the final evaluation section\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Prepare the final evaluation log entry\n",
    "    final_log_entry = (f\"\\nFinal Evaluation on Test Set:\\n\"\n",
    "                       f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "                       f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\\n\"\n",
    "                       f\"\\n=======================================================================================================================\\n\")\n",
    "    \n",
    "    # Append final evaluation to the log file\n",
    "    with open(log_filename, 'a') as log_file:\n",
    "        log_file.write(final_log_entry)\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Load and Prepare the IMDB Dataset\n",
    "# -------------------------------\n",
    "print(\"Loading IMDB dataset...\")\n",
    "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
    "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
    "                                           split=['train', 'test'],\n",
    "                                           as_supervised=True,\n",
    "                                           with_info=True)\n",
    "\n",
    "# Convert training dataset to lists.\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for text, label in tfds.as_numpy(ds_train):\n",
    "    # Decode byte strings to utf-8 strings.\n",
    "    train_texts.append(text.decode('utf-8'))\n",
    "    train_labels.append(label)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Create a validation set from the training data (20% for validation).\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert test dataset to lists.\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for text, label in tfds.as_numpy(ds_test):\n",
    "    test_texts.append(text.decode('utf-8'))\n",
    "    test_labels.append(label)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char tokenizer vocabulary size: 134\n",
      "Word tokenizer vocabulary size: 80169\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Preprocessing: Tokenization and Vectorization\n",
    "# -------------------------------\n",
    "# Build the character-level tokenizer on the training texts.\n",
    "char_tokenizer = create_tokenizer(train_texts, True)\n",
    "print(\"Char tokenizer vocabulary size:\", len(char_tokenizer.word_index) + 1)\n",
    "\n",
    "word_tokenizer = create_tokenizer(train_texts, False)\n",
    "print(\"Word tokenizer vocabulary size:\", len(word_tokenizer.word_index) + 1)\n",
    "\n",
    "\n",
    "# Convert texts to bag-of-characters representation.\n",
    "X_train = texts_to_bow(char_tokenizer, train_texts)\n",
    "X_val   = texts_to_bow(char_tokenizer, val_texts)\n",
    "X_test  = texts_to_bow(char_tokenizer, test_texts)\n",
    "\n",
    "# Convert labels to one-hot encoding.\n",
    "y_train = one_hot_encode(train_labels)\n",
    "y_val   = one_hot_encode(val_labels)\n",
    "y_test  = one_hot_encode(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Defining the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Original MLP Class Definition\n",
    "# -------------------------------\n",
    "class MLP(object):\n",
    "    def __init__(self, size_input, num_hidden, hidden_sizes, size_output, batch_size, optimizer, learning_rate, act_func, device=None):\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden1: int, size of the 1st hidden layer\n",
    "        size_hidden2: int, size of the 2nd hidden layer\n",
    "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None.\n",
    "        \"\"\"\n",
    "        self.size_input = size_input\n",
    "        self.num_hidden = num_hidden\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        # self.size_hidden1 = size_hidden1\n",
    "        # self.size_hidden2 = size_hidden2\n",
    "        # self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
    "        self.size_output = size_output\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.act_func = act_func\n",
    "        \n",
    "        self.hidden_layers = []\n",
    "        \n",
    "        w = tf.Variable(tf.random.normal([self.size_input, self.hidden_sizes[0]], stddev=0.1))\n",
    "        b = tf.Variable(tf.zeros([1, self.hidden_sizes[0]]))\n",
    "        self.hidden_layers.append((w, b))\n",
    "        \n",
    "        for i in range(1, self.num_hidden):\n",
    "            w = tf.Variable(tf.random.normal([self.hidden_sizes[i-1], self.hidden_sizes[i]], stddev=0.1))\n",
    "            b = tf.Variable(tf.zeros([1, self.hidden_sizes[i]]))\n",
    "            self.hidden_layers.append((w, b))\n",
    "        \n",
    "        w = tf.Variable(tf.random.normal([self.hidden_sizes[-1], self.size_output], stddev=0.1))\n",
    "        b = tf.Variable(tf.zeros([1, self.size_output]))\n",
    "        self.hidden_layers.append((w, b))\n",
    "        \n",
    "\n",
    "        # # Initialize weights and biases for first hidden layer\n",
    "        # self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
    "        # self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
    "\n",
    "        # # Initialize weights and biases for second hidden layer\n",
    "        # self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
    "        # self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
    "\n",
    "        # # Initialize weights and biases for third hidden layer\n",
    "        # self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3], stddev=0.1))\n",
    "        # self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
    "        \n",
    "        # # Initialize weights and biases for output layer\n",
    "        # self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output], stddev=0.1))\n",
    "        # self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
    "\n",
    "        # # List of variables to update during backpropagation\n",
    "        # self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        X: Tensor, inputs.\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X)\n",
    "        else:\n",
    "            self.y = self.compute_output(X)\n",
    "        return self.y\n",
    "\n",
    "    def loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the loss between predicted and true outputs.\n",
    "        y_pred: Tensor of shape (batch_size, size_output)\n",
    "        y_true: Tensor of shape (batch_size, size_output)\n",
    "        \"\"\"\n",
    "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        loss_x = cce(y_true_tf, y_pred_tf)\n",
    "        return loss_x\n",
    "\n",
    "    def backward(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Backward pass: compute and apply gradients of the loss with respect to the variables.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train)\n",
    "            current_loss = self.loss(predicted, y_train)\n",
    "        \n",
    "        trainable_vars = [var for w, b in self.hidden_layers for var in (w, b)]    \n",
    "        \n",
    "        grads = tape.gradient(current_loss, trainable_vars)\n",
    "        \n",
    "        optimizer = self.optimizer(learning_rate=self.learning_rate)\n",
    "        optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "\n",
    "    def compute_output(self, X):\n",
    "        \"\"\"\n",
    "        Custom method to compute the output tensor during the forward pass.\n",
    "        \"\"\"        \n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        z = X_tf\n",
    "        \n",
    "        for w, b in self.hidden_layers:\n",
    "            z = tf.matmul(z, w) + b\n",
    "            \n",
    "            if (w, b) == self.hidden_layers[-1]:\n",
    "                return z\n",
    "            \n",
    "            z = self.act_func(z)\n",
    "            \n",
    "        \n",
    "        # output = tf.matmul(z)\n",
    "        \n",
    "        # # First hidden layer\n",
    "        # h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
    "        # z1 = self.act_func(h1)\n",
    "        \n",
    "        # # Second hidden layer\n",
    "        # h2 = tf.matmul(z1, self.W2) + self.b2\n",
    "        # z2 = self.act_func(h2)\n",
    "\n",
    "        # # Third hidden layer\n",
    "        # h3 = tf.matmul(z2, self.W3) + self.b3\n",
    "        # z3 = self.act_func(h3)\n",
    "\n",
    "        # # Output layer (logits)\n",
    "        # output = tf.matmul(z3, self.W4) + self.b4\n",
    "        # return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "2\n",
      "[128, 128]\n",
      "64\n",
      "<class 'keras.src.optimizers.sgd.SGD'>\n",
      "<function relu at 0x707dd859b910>\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6992 | Val Loss: 0.6939 | Accuracy: 0.5052 | Precision: 0.4888 | Recall: 0.4517\n",
      "Epoch 02 | Training Loss: 0.6929 | Val Loss: 0.6922 | Accuracy: 0.5160 | Precision: 0.5008 | Recall: 0.5281\n",
      "Epoch 03 | Training Loss: 0.6910 | Val Loss: 0.6906 | Accuracy: 0.5294 | Precision: 0.5128 | Recall: 0.5862\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.6906 | Test Accuracy: 0.5322 | Test Precision: 0.5295 | Test Recall: 0.5778\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6892 | Val Loss: 0.6885 | Accuracy: 0.5388 | Precision: 0.5259 | Recall: 0.4934\n",
      "Epoch 02 | Training Loss: 0.6876 | Val Loss: 0.6868 | Accuracy: 0.5486 | Precision: 0.5368 | Recall: 0.5025\n",
      "Epoch 03 | Training Loss: 0.6860 | Val Loss: 0.6858 | Accuracy: 0.5620 | Precision: 0.5421 | Recall: 0.6221\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.6857 | Test Accuracy: 0.5636 | Test Precision: 0.5589 | Test Recall: 0.6034\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 01 | Training Loss: 0.6846 | Val Loss: 0.6843 | Accuracy: 0.5684 | Precision: 0.5478 | Recall: 0.6291\n",
      "Epoch 02 | Training Loss: 0.6832 | Val Loss: 0.6831 | Accuracy: 0.5748 | Precision: 0.5522 | Recall: 0.6498\n",
      "Epoch 03 | Training Loss: 0.6818 | Val Loss: 0.6817 | Accuracy: 0.5800 | Precision: 0.5590 | Recall: 0.6328\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Loss: 0.6816 | Test Accuracy: 0.5804 | Test Precision: 0.5750 | Test Recall: 0.6166\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Model Setup\n",
    "# -------------------------------\n",
    "# The input size is determined by the dimension of the bag-of-characters vector.\n",
    "size_input = X_train.shape[1]\n",
    "\n",
    "# Set hidden layer sizes as desired.\n",
    "# hidden_sizes = [128, 64, 32]\n",
    "# size_hidden1 = 128\n",
    "# size_hidden2 = 64\n",
    "# size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
    "# size_output  = 2\n",
    "\n",
    "# Instantiate the MLP model.\n",
    "# model = MLP(size_input,\n",
    "#             num_hidden=3,\n",
    "#             hidden_sizes=[128, 64, 32],\n",
    "#             size_output=2,\n",
    "#             batch_size=128,\n",
    "#             optimizer=tf.keras.optimizers.Adam,\n",
    "#             learning_rate=0.001,\n",
    "#             act_func=tf.nn.relu)\n",
    "\n",
    "learning_rate, num_layers, hidden_sizes, batch_size, optimizer, act_func = sample_hyperparameters()\n",
    "size_output  = 2\n",
    "\n",
    "print(learning_rate, num_layers, hidden_sizes, batch_size, optimizer, act_func, sep=\"\\n\")\n",
    "\n",
    "model = MLP(size_input=size_input,\n",
    "            num_hidden=num_layers,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            size_output=size_output,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=optimizer,\n",
    "            learning_rate=learning_rate,\n",
    "            act_func=act_func,\n",
    "            device=None)\n",
    "\n",
    "# Define the optimizer.\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.001).apply_gradients(zip(grads, model.variables))\n",
    "\n",
    "# -------------------------------\n",
    "# Training Parameters and Loop\n",
    "# -------------------------------\n",
    "# batch_size = 128\n",
    "for i in range(3):\n",
    "    epochs = 3\n",
    "    num_batches = int(np.ceil(X_train.shape[0] / model.batch_size))\n",
    "\n",
    "    print(\"\\nStarting training...\\n\")\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data at the start of each epoch.\n",
    "        indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            start = i * model.batch_size\n",
    "            end = min((i+1) * model.batch_size, X_train.shape[0])\n",
    "            X_batch = X_train[start:end]\n",
    "            y_batch = y_train[start:end]\n",
    "\n",
    "            # Compute gradients and update weights.\n",
    "            # with tf.GradientTape() as tape:\n",
    "            #     predictions = model.forward(X_batch)\n",
    "            #     loss_value = model.loss(predictions, y_batch)\n",
    "            # grads = tape.gradient(loss_value, model.variables)\n",
    "            predictions = model.forward(X_batch)\n",
    "            loss_value = model.loss(predictions, y_batch)\n",
    "            model.backward(X_batch, y_batch)\n",
    "            #grads = model.backward(X_batch, y_batch)\n",
    "            #optimizer.apply_gradients(zip(grads, model.variables))\n",
    "            epoch_loss += loss_value.numpy() * (end - start)\n",
    "\n",
    "        epoch_loss /= X_train.shape[0]\n",
    "\n",
    "        # Evaluate on validation set.\n",
    "        val_logits = model.forward(X_val)\n",
    "        val_loss = model.loss(val_logits, y_val).numpy()\n",
    "        val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
    "        true_val = np.argmax(y_val, axis=1)\n",
    "        accuracy = np.mean(val_preds == true_val)\n",
    "        precision = precision_score(true_val, val_preds)\n",
    "        recall = recall_score(true_val, val_preds)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "            f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "        \n",
    "        log_training_info(epoch, epoch_loss, val_loss, accuracy, precision, recall)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Final Evaluation on Test Set\n",
    "    # -------------------------------\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_logits = model.forward(X_test)\n",
    "    test_loss = model.loss(test_logits, y_test).numpy()\n",
    "    test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
    "    true_test = np.argmax(y_test, axis=1)\n",
    "    test_accuracy = np.mean(test_preds == true_test)\n",
    "    test_precision = precision_score(true_test, test_preds)\n",
    "    test_recall = recall_score(true_test, test_preds)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
    "        f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "    log_final_evaluation(test_loss, test_accuracy, test_precision, test_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
